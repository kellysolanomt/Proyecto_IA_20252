{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b78ff95a",
      "metadata": {},
      "source": [
        "# **Proyecto Final Inteligencia Artificial**\n",
        "\n",
        "### Autores: **Angel David Piñeros Sierra**, **Camilo Andrés Roncancio Toca**, **Kelly Johana Solano Calderón**\n",
        "### Presentado a: **Darwin Eduardo Martinez Riaño**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a410ac",
      "metadata": {},
      "source": [
        "## **Modelo de segmentación de imágenes para la localización de lesiones asociadas al cáncer de piel**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb7b245c",
      "metadata": {},
      "source": [
        "### *Glosario*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7598b777",
      "metadata": {},
      "source": [
        "### *(A) Descripción de la problemática*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a0a7714",
      "metadata": {},
      "source": [
        "### *(B) Objetivo*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca1cd0cb",
      "metadata": {},
      "source": [
        "### *(C) Descripción del dataset*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa109c33",
      "metadata": {},
      "source": [
        "El dataset seleccionado para la evaluación del modelo fue el denominado “Skin cancer: HAM10000” de la plataforma de Kaggle, el cual ofrece un conjunto de imágenes especiales para realizar tareas de segmentación y clasificación. Para el propósito de segmentación, el dataset incluye para cada una de las imágenes, el conjunto de máscaras qué determinan la segmentación de las lesiones de cáncer de piel. \n",
        "\n",
        "> El acrónimo HAM10000 significa “Human Against Machine with 10000 training images”. \n",
        "\n",
        "Este dataset es una recopilación de imágenes demoscópicas de diferentes poblaciones. Estas fueron originalmente publicadas inicialmente en el repositorio de Harvard Dataverse,  con el propósito de abordar la dificultad de encontrar un dataset lo suficientemente grande y diverso para realizar diagnósticos automatizados de lesiones cutáneas pigmentadas. \n",
        "\n",
        "El dataset se conforma de dos carpetas: images y masks. Cada una con **10015** imágenes en formato **.JPEG**. Todas las imágenes tienen una dimensión de `600px X 450px`\n",
        "\n",
        "<img src=\"https://res.cloudinary.com/dlsntlruu/image/upload/v1764556079/carpeta_images_pieoyu.png\" width=\"600px\"/>\n",
        "\n",
        "<img src=\"https://res.cloudinary.com/dlsntlruu/image/upload/v1764556079/carpeta_masks_taifwn.png\" width=\"600px\"/>\n",
        "\n",
        "Las imágenes incluyen diagnósticos de:\n",
        "*  Queratosis actínicas\n",
        "*  Carcinoma intraepitelial\n",
        "*  Carcinoma basocelular\n",
        "*  Lesiones de tipo queratosis\n",
        "*  Dermatofibroma\n",
        "*  Melanoma\n",
        "*  Lesiones vasculares\n",
        "\n",
        "Contar con una amplia gama de diagnósticos permite qué la tarea de segmentación semántica pueda realizarse de forma óptima. \n",
        "\n",
        "Para mayor información: \n",
        "\n",
        "*  Skin cancer: HAM10000: https://www.kaggle.com/datasets/surajghuwalewala/ham1000-segmentation-and-classification/\n",
        "*  The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "313944b8",
      "metadata": {},
      "source": [
        "### *(D) Importación y organización de datos*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "558cb2b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (1.8.2)\n",
            "Requirement already satisfied: pandas in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (2.3.3)\n",
            "Requirement already satisfied: torch in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (2.9.1)\n",
            "Requirement already satisfied: PILlow in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (12.0.0)\n",
            "Requirement already satisfied: torchvision in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (0.24.1)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: black>=24.10.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (25.11.0)\n",
            "Requirement already satisfied: bleach in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: kagglesdk in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (0.1.13)\n",
            "Requirement already satisfied: mypy>=1.15.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (1.19.0)\n",
            "Requirement already satisfied: protobuf in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (6.33.1)\n",
            "Requirement already satisfied: python-dateutil in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.32.5)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (80.9.0)\n",
            "Requirement already satisfied: six>=1.10 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: types-requests in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.32.4.20250913)\n",
            "Requirement already satisfied: types-tqdm in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (4.67.0.20250809)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from pandas) (2.3.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.5.1)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.61.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (113 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
            "Collecting pyparsing>=3 (from matplotlib)\n",
            "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (8.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (1.1.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (4.5.0)\n",
            "Requirement already satisfied: pytokens>=0.3.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (0.3.0)\n",
            "Requirement already satisfied: librt>=0.6.2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from mypy>=1.15.0->kaggle) (0.6.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: webencodings in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from requests->kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from requests->kaggle) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from requests->kaggle) (2025.11.12)\n",
            "Using cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.61.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
            "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [matplotlib]6\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.0 kiwisolver-1.4.9 matplotlib-3.10.7 pyparsing-3.2.5\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle pandas torch PILlow torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "453f4670",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.v2 as v2\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "c8077e39",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La carpeta 'ham1000_data' ya existe. No se descargará de nuevo.\n",
            "Contenido de la carpeta: ['GroundTruth.csv', 'images', 'masks']\n",
            "Número de imágenes: 10017\n",
            "Número de máscaras: 10015\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "dataset_url = \"surajghuwalewala/ham1000-segmentation-and-classification\"\n",
        "ruta_descarga = \"ham1000_data\"\n",
        "\n",
        "if os.path.exists(ruta_descarga):\n",
        "    print(f\"La carpeta '{ruta_descarga}' ya existe. No se descargará de nuevo.\")\n",
        "else:\n",
        "    os.makedirs(ruta_descarga, exist_ok=True)\n",
        "    subprocess.run([\n",
        "        \"kaggle\", \"datasets\", \"download\",\n",
        "        \"-d\", dataset_url,\n",
        "        \"-p\", ruta_descarga,\n",
        "        \"--unzip\"\n",
        "    ], check=True)\n",
        "\n",
        "print(\"Contenido de la carpeta:\", os.listdir(ruta_descarga))\n",
        "\n",
        "images_dir = os.path.join(ruta_descarga, \"images\")\n",
        "masks_dir = os.path.join(ruta_descarga, \"masks\")\n",
        "\n",
        "if os.path.isdir(images_dir):\n",
        "    print(\"Número de imágenes:\", len(os.listdir(images_dir)))\n",
        "else:\n",
        "    print(\"No encontré la carpeta 'images'.\")\n",
        "\n",
        "if os.path.isdir(masks_dir):\n",
        "    print(\"Número de máscaras:\", len(os.listdir(masks_dir)))\n",
        "else:\n",
        "    print(\"No encontré la carpeta 'masks'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c09ae392",
      "metadata": {
        "tags": [
          "splits"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 7236 imágenes\n",
            "val: 1276 imágenes\n",
            "test: 1503 imágenes\n",
            "Splits creados en ham1000_data_splits\n"
          ]
        }
      ],
      "source": [
        "DATA_ROOT = Path('ham1000_data')\n",
        "OUTPUT_ROOT = Path('ham1000_data_splits')\n",
        "TRAIN_RATIO = 0.85  # train + val\n",
        "VAL_FROM_TRAIN = 0.15  # porcentaje de train destinado a validación\n",
        "SEED = 42\n",
        "MASK_SUFFIX = '_segmentation.png'\n",
        "\n",
        "image_dir = DATA_ROOT / 'images'\n",
        "mask_dir = DATA_ROOT / 'masks'\n",
        "if not image_dir.exists() or not mask_dir.exists():\n",
        "    raise RuntimeError(f\"No se hallaron carpetas esperadas en {DATA_ROOT}.\")\n",
        "\n",
        "random.seed(SEED)\n",
        "if OUTPUT_ROOT.exists():\n",
        "    shutil.rmtree(OUTPUT_ROOT)\n",
        "for split in ('train', 'val', 'test'):\n",
        "    (OUTPUT_ROOT / split / 'images').mkdir(parents=True, exist_ok=True)\n",
        "    (OUTPUT_ROOT / split / 'masks').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "allowed_exts = {'.png', '.jpg', '.jpeg'}\n",
        "image_paths = [p for p in image_dir.iterdir() if p.suffix.lower() in allowed_exts]\n",
        "if not image_paths:\n",
        "    raise RuntimeError(f'No se encontraron imágenes en {image_dir}.')\n",
        "image_paths.sort()\n",
        "random.shuffle(image_paths)\n",
        "\n",
        "total = len(image_paths)\n",
        "train_val_count = int(total * TRAIN_RATIO)\n",
        "val_count = int(train_val_count * VAL_FROM_TRAIN)\n",
        "train_count = train_val_count - val_count\n",
        "\n",
        "splits = {\n",
        "    'train': image_paths[:train_count],\n",
        "    'val': image_paths[train_count: train_count + val_count],\n",
        "    'test': image_paths[train_count + val_count:]\n",
        "}\n",
        "\n",
        "for split_name, files in splits.items():\n",
        "    dst_img = OUTPUT_ROOT / split_name / 'images'\n",
        "    dst_mask = OUTPUT_ROOT / split_name / 'masks'\n",
        "    for img_path in files:\n",
        "        mask_path = mask_dir / f\"{img_path.stem}{MASK_SUFFIX}\"\n",
        "        if not mask_path.exists():\n",
        "            raise FileNotFoundError(f'Falta máscara: {mask_path}')\n",
        "        shutil.copy2(img_path, dst_img / img_path.name)\n",
        "        shutil.copy2(mask_path, dst_mask / mask_path.name)\n",
        "    print(f\"{split_name}: {len(files)} imágenes\")\n",
        "print('Splits creados en', OUTPUT_ROOT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e1e979a2",
      "metadata": {
        "tags": [
          "train_stats"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Media por canal (reescala 256x256): [0.7633206844329834, 0.5454925894737244, 0.5698098540306091]\n",
            "Desviación estándar por canal: [0.08873128145933151, 0.11731848120689392, 0.13180507719516754]\n"
          ]
        }
      ],
      "source": [
        "TRAIN_IMG_DIR = Path('ham1000_data_splits/train/images')\n",
        "if not TRAIN_IMG_DIR.exists():\n",
        "    raise RuntimeError(f'No existe el directorio: {TRAIN_IMG_DIR}')\n",
        "\n",
        "TARGET_SIZE = (256, 256)\n",
        "resize_transform = transforms.Resize(TARGET_SIZE, interpolation=InterpolationMode.BILINEAR)\n",
        "\n",
        "img_files = sorted(\n",
        "    [p for p in TRAIN_IMG_DIR.iterdir() if p.suffix.lower() in {'.png', '.jpg', '.jpeg'}]\n",
        ")\n",
        "if not img_files:\n",
        "    raise RuntimeError(f'No se encontraron imágenes en {TRAIN_IMG_DIR}')\n",
        "\n",
        "means, stds = [], []\n",
        "for img_path in img_files:\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_tensor = transforms.ToTensor()(resize_transform(img))\n",
        "    means.append(img_tensor.mean(dim=(1, 2)))\n",
        "    stds.append(img_tensor.std(dim=(1, 2)))\n",
        "\n",
        "IMG_MEAN = torch.stack(means).mean(dim=0)\n",
        "IMG_STD = torch.stack(stds).mean(dim=0)\n",
        "print(f'Media por canal (reescala {TARGET_SIZE[0]}x{TARGET_SIZE[1]}): {IMG_MEAN.tolist()}')\n",
        "print(f'Desviación estándar por canal: {IMG_STD.tolist()}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "0461c7f5",
      "metadata": {
        "tags": [
          "transforms"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformaciones definidas:\n",
            "train_joint_transform: Compose(\n",
            "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
            "      RandomHorizontalFlip(p=0.5)\n",
            "      RandomVerticalFlip(p=0.2)\n",
            "      RandomRotation(degrees=[-180.0, 180.0], interpolation=InterpolationMode.NEAREST, expand=False, fill=0)\n",
            "      ElasticTransform(alpha=[40.0, 40.0], sigma=[5.0, 5.0], interpolation=InterpolationMode.BILINEAR, fill=0)\n",
            "      ColorJitter(brightness=(0.85, 1.15), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.02, 0.02))\n",
            "      GaussianBlur(kernel_size=(3, 3), sigma=[0.1, 1.0])\n",
            "      ToDtype(scale=True)\n",
            "      Normalize(mean=[tensor(0.7633), tensor(0.5455), tensor(0.5698)], std=[tensor(0.0887), tensor(0.1173), tensor(0.1318)], inplace=False)\n",
            ")\n",
            "val_test_joint_transform: Compose(\n",
            "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
            "      ToDtype(scale=True)\n",
            "      Normalize(mean=[tensor(0.7633), tensor(0.5455), tensor(0.5698)], std=[tensor(0.0887), tensor(0.1173), tensor(0.1318)], inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "normalize_transform = v2.Normalize(mean=IMG_MEAN, std=IMG_STD)\n",
        "\n",
        "train_joint_transform = v2.Compose([\n",
        "    v2.Resize(TARGET_SIZE, interpolation=InterpolationMode.BILINEAR, antialias=True),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.RandomVerticalFlip(p=0.2),\n",
        "    v2.RandomRotation(degrees=(-180, 180)),\n",
        "    v2.ElasticTransform(alpha=40.0, sigma=5.0, interpolation=InterpolationMode.BILINEAR),\n",
        "    v2.ColorJitter(brightness=0.15, contrast=0.2, saturation=0.2, hue=0.02),\n",
        "    v2.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    normalize_transform,\n",
        "])\n",
        "\n",
        "val_test_joint_transform = v2.Compose([\n",
        "    v2.Resize(TARGET_SIZE, interpolation=InterpolationMode.BILINEAR, antialias=True),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    normalize_transform,\n",
        "])\n",
        "\n",
        "def apply_train_transforms(image, mask):\n",
        "    image_tv = tv_tensors.Image(image)\n",
        "    mask_tv = tv_tensors.Mask(mask)\n",
        "    image_aug, mask_aug = train_joint_transform(image_tv, mask_tv)\n",
        "    return image_aug, mask_aug\n",
        "\n",
        "def apply_val_transforms(image, mask):\n",
        "    image_tv = tv_tensors.Image(image)\n",
        "    mask_tv = tv_tensors.Mask(mask)\n",
        "    image_val, mask_val = val_test_joint_transform(image_tv, mask_tv)\n",
        "    return image_val, mask_val\n",
        "\n",
        "def apply_test_transforms(image, mask):\n",
        "    image_tv = tv_tensors.Image(image)\n",
        "    mask_tv = tv_tensors.Mask(mask)\n",
        "    image_test, mask_test = val_test_joint_transform(image_tv, mask_tv)\n",
        "    return image_test, mask_test\n",
        "\n",
        "print(\"Transformaciones definidas:\")\n",
        "print(\"train_joint_transform:\", train_joint_transform)\n",
        "print(\"val_test_joint_transform:\", val_test_joint_transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "c73fa70f",
      "metadata": {
        "tags": [
          "dataset"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 7236 muestras\n",
            "val: 1276 muestras\n",
            "test: 1503 muestras\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class HAM1000SegmentationDataset(Dataset):\n",
        "    def __init__(self, root_dir: Path, split: str, transform_fn=None, mask_suffix: str = '_segmentation.png'):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        if split not in {'train', 'val', 'test'}:\n",
        "            raise ValueError(\"split debe ser 'train', 'val' o 'test'\")\n",
        "        self.split = split\n",
        "        self.images_dir = self.root_dir / split / 'images'\n",
        "        self.masks_dir = self.root_dir / split / 'masks'\n",
        "        if not self.images_dir.exists() or not self.masks_dir.exists():\n",
        "            raise RuntimeError(f'No se hallan carpetas para el split {split} en {self.root_dir}')\n",
        "        self.mask_suffix = mask_suffix\n",
        "        self.transform_fn = transform_fn\n",
        "\n",
        "        allowed_exts = {'.png', '.jpg', '.jpeg'}\n",
        "        self.samples = []\n",
        "        for img_path in sorted(self.images_dir.iterdir()):\n",
        "            if img_path.suffix.lower() not in allowed_exts:\n",
        "                continue\n",
        "            mask_path = self.masks_dir / f\"{img_path.stem}{self.mask_suffix}\"\n",
        "            if not mask_path.exists():\n",
        "                continue\n",
        "            self.samples.append((img_path, mask_path))\n",
        "        if not self.samples:\n",
        "            raise RuntimeError(f'No se encontraron pares imagen-máscara en {self.images_dir}')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        if self.transform_fn is not None:\n",
        "            image, mask = self.transform_fn(image, mask)\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "def get_transform_fn(split: str):\n",
        "    if split == 'train':\n",
        "        return apply_train_transforms\n",
        "    if split == 'val':\n",
        "        return apply_val_transforms\n",
        "    if split == 'test':\n",
        "        return apply_test_transforms\n",
        "    raise ValueError('split desconocido')\n",
        "\n",
        "\n",
        "def create_datasets(root_dir: Path = Path('ham1000_data_splits')):\n",
        "    datasets = {}\n",
        "    for split in ('train', 'val', 'test'):\n",
        "        datasets[split] = HAM1000SegmentationDataset(\n",
        "            root_dir=root_dir,\n",
        "            split=split,\n",
        "            transform_fn=get_transform_fn(split),\n",
        "        )\n",
        "        print(f\"{split}: {len(datasets[split])} muestras\")\n",
        "    return datasets\n",
        "\n",
        "ham_datasets = create_datasets()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "add8745d",
      "metadata": {
        "tags": [
          "dataloader"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaders listos:\n",
            "train: 905 batches\n",
            "val: 160 batches\n",
            "test: 188 batches\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 4\n",
        "PIN_MEMORY = True\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ham_datasets['train'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    ham_datasets['val'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    ham_datasets['test'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        ")\n",
        "\n",
        "print('Loaders listos:')\n",
        "print(f\"train: {len(train_loader)} batches\")\n",
        "print(f\"val: {len(val_loader)} batches\")\n",
        "print(f\"test: {len(test_loader)} batches\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "a4873482",
      "metadata": {
        "tags": [
          "model"
        ]
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Two Conv-BN blocks with a residual shortcut.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = out + identity\n",
        "        return F.relu(out)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"MaxPool + ResidualBlock encoder stage.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.resblock = ResidualBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(x)\n",
        "        x = self.resblock(x)\n",
        "        return x\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"ConvTranspose2d upsampling + concat + ResidualBlock.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.resblock = ResidualBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diff_y = x2.size(2) - x1.size(2)\n",
        "        diff_x = x2.size(3) - x1.size(3)\n",
        "        if diff_y != 0 or diff_x != 0:\n",
        "            x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2, diff_y // 2, diff_y - diff_y // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.resblock(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class ResUNet(nn.Module):\n",
        "    def __init__(self, n_channels=3, n_classes=2):\n",
        "        super().__init__()\n",
        "        self.inc = ResidualBlock(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        return self.outc(x)\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "5434279d",
      "metadata": {
        "tags": [
          "metrics"
        ]
      },
      "outputs": [],
      "source": [
        "def _ensure_multiclass_logits(preds):\n",
        "    if preds.ndim != 4:\n",
        "        raise ValueError('preds debe tener forma [B, C, H, W]')\n",
        "    if preds.shape[1] == 1:\n",
        "        probs = torch.sigmoid(preds)\n",
        "        return torch.cat([1.0 - probs, probs], dim=1)\n",
        "    return preds\n",
        "\n",
        "def pixel_accuracy(preds, targets):\n",
        "    preds = _ensure_multiclass_logits(preds)\n",
        "    pred_labels = torch.argmax(preds, dim=1)\n",
        "    correct = (pred_labels == targets).float().sum()\n",
        "    total = torch.numel(targets)\n",
        "    return (correct / total).item()\n",
        "\n",
        "def _per_class_iou(pred_labels, targets, num_classes=2, eps=1e-6):\n",
        "    per_class = {}\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = pred_labels == cls\n",
        "        target_cls = targets == cls\n",
        "        intersection = (pred_cls & target_cls).float().sum()\n",
        "        union = (pred_cls | target_cls).float().sum()\n",
        "        if union == 0:\n",
        "            continue\n",
        "        per_class[cls] = ((intersection + eps) / (union + eps)).item()\n",
        "    return per_class\n",
        "\n",
        "def _per_class_dice(pred_labels, targets, num_classes=2, eps=1e-6):\n",
        "    per_class = {}\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = pred_labels == cls\n",
        "        target_cls = targets == cls\n",
        "        intersection = (pred_cls & target_cls).float().sum()\n",
        "        pred_sum = pred_cls.float().sum()\n",
        "        target_sum = target_cls.float().sum()\n",
        "        denom = pred_sum + target_sum\n",
        "        if denom == 0:\n",
        "            continue\n",
        "        per_class[cls] = ((2 * intersection + eps) / (denom + eps)).item()\n",
        "    return per_class\n",
        "\n",
        "def compute_jaccard(preds, targets, num_classes=2, eps=1e-6):\n",
        "    preds = _ensure_multiclass_logits(preds)\n",
        "    pred_labels = torch.argmax(preds, dim=1)\n",
        "    per_class = _per_class_iou(pred_labels, targets, num_classes, eps)\n",
        "    if not per_class:\n",
        "        return {'mean': 0.0, 'per_class': per_class}\n",
        "    mean_score = sum(per_class.values()) / len(per_class)\n",
        "    return {'mean': mean_score, 'per_class': per_class}\n",
        "\n",
        "def compute_dice(preds, targets, num_classes=2, eps=1e-6):\n",
        "    preds = _ensure_multiclass_logits(preds)\n",
        "    pred_labels = torch.argmax(preds, dim=1)\n",
        "    per_class = _per_class_dice(pred_labels, targets, num_classes, eps)\n",
        "    if not per_class:\n",
        "        return {'mean': 0.0, 'per_class': per_class}\n",
        "    mean_score = sum(per_class.values()) / len(per_class)\n",
        "    return {'mean': mean_score, 'per_class': per_class}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "f5be89b9",
      "metadata": {
        "tags": [
          "model_load"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo cargado desde resunet_gsd_lr5e4.pt\n",
            "Dispositivo: cpu\n",
            "Keys ignoradas: ['outc.conv.weight', 'outc.conv.bias']\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "WEIGHTS_PATH = Path('resunet_gsd_lr5e4.pt')\n",
        "device = get_device()\n",
        "model = ResUNet().to(device)\n",
        "\n",
        "checkpoint = torch.load(WEIGHTS_PATH, map_location=device)\n",
        "state_dict = (\n",
        "    checkpoint.get('state_dict')\n",
        "    or checkpoint.get('model_state_dict')\n",
        "    or checkpoint\n",
        ")\n",
        "for key in ('outc.conv.weight', 'outc.conv.bias'):\n",
        "    state_dict.pop(key, None)\n",
        "missing = model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "print(f'Modelo cargado desde {WEIGHTS_PATH}')\n",
        "print('Dispositivo:', device)\n",
        "print('Keys ignoradas:', missing.missing_keys)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "fc3343b2",
      "metadata": {
        "tags": [
          "hparam_search"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000163\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "------------------------------\n",
            "Trial 2:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000103\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 20}\n",
            "------------------------------\n",
            "Trial 3:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000112\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "------------------------------\n",
            "Trial 4:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000214\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.1}\n",
            "------------------------------\n",
            "Trial 5:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.001675\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0\n",
            "  scheduler: steplr\n",
            "  scheduler_params: {'step_size': 30, 'gamma': 0.1}\n",
            "------------------------------\n",
            "Trial 6:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000133\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "------------------------------\n",
            "Trial 7:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000912\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 20}\n",
            "------------------------------\n",
            "Trial 8:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000215\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "------------------------------\n",
            "Trial 9:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.00063\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 20}\n",
            "------------------------------\n",
            "Trial 10:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000115\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.1}\n",
            "------------------------------\n",
            "Trial 11:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.001599\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "------------------------------\n",
            "Trial 12:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 5.2e-05\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 20}\n",
            "------------------------------\n",
            "Trial 13:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000325\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "------------------------------\n",
            "Trial 14:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000201\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0\n",
            "  scheduler: steplr\n",
            "  scheduler_params: {'step_size': 15, 'gamma': 0.1}\n",
            "------------------------------\n",
            "Trial 15:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000396\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "------------------------------\n",
            "Trial 16:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000164\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "------------------------------\n",
            "Trial 17:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000136\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.3}\n",
            "------------------------------\n",
            "Trial 18:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.040251\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 5, 'factor': 0.1}\n",
            "  momentum: 0.99\n",
            "------------------------------\n",
            "Trial 19:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000629\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "------------------------------\n",
            "Trial 20:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000656\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.1}\n",
            "------------------------------\n",
            "Trial 21:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000143\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.1}\n",
            "------------------------------\n",
            "Trial 22:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.024266\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.3}\n",
            "  momentum: 0.9\n",
            "------------------------------\n",
            "Trial 23:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000122\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "------------------------------\n",
            "Trial 24:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000925\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.1}\n",
            "------------------------------\n",
            "Trial 25:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.039404\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "  momentum: 0.99\n",
            "------------------------------\n",
            "Trial 26:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000182\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 20}\n",
            "------------------------------\n",
            "Trial 27:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.009168\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "  momentum: 0.99\n",
            "------------------------------\n",
            "Trial 28:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000745\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.3}\n",
            "------------------------------\n",
            "Trial 29:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.018989\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0\n",
            "  scheduler: steplr\n",
            "  scheduler_params: {'step_size': 30, 'gamma': 0.1}\n",
            "  momentum: 0.9\n",
            "------------------------------\n",
            "Trial 30:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.018748\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "  momentum: 0.9\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "random.seed(45)\n",
        "\n",
        "OPTIMIZERS = ('adam', 'adamw', 'sgd')\n",
        "BATCH_CHOICES = (8, 12, 16)\n",
        "WEIGHT_DECAY_VALUES = (0.0, 1e-4, 5e-4)\n",
        "MOMENTUM_VALUES = (0.85, 0.9, 0.95, 0.99)\n",
        "SCHEDULERS = {\n",
        "    'none': {},\n",
        "    'steplr': {'step_size': (15, 30), 'gamma': (0.5, 0.1)},\n",
        "    'cosine': {'t_max': (20, 40)},\n",
        "    'plateau': {'patience': (3, 5), 'factor': (0.3, 0.1)},\n",
        "}\n",
        "\n",
        "def sample_lr(opt_name):\n",
        "    if opt_name == 'sgd':\n",
        "        return 10 ** random.uniform(-2.3, -1.5)\n",
        "    if opt_name == 'adamw':\n",
        "        return 10 ** random.uniform(-4.3, -3.0)\n",
        "    return 10 ** random.uniform(-4.0, -3.0)\n",
        "\n",
        "def sample_scheduler_params(name):\n",
        "    params = {}\n",
        "    cfg = SCHEDULERS[name]\n",
        "    for key, bounds in cfg.items():\n",
        "        params[key] = random.choice(bounds)\n",
        "    return params\n",
        "\n",
        "def sample_trial():\n",
        "    opt_name = random.choice(OPTIMIZERS)\n",
        "    batch_size = random.choice(BATCH_CHOICES)\n",
        "    weight_decay = random.choice(WEIGHT_DECAY_VALUES)\n",
        "    lr = sample_lr(opt_name) * (batch_size / 8)\n",
        "    scheduler = random.choice(tuple(SCHEDULERS.keys()))\n",
        "    trial = {\n",
        "        'optimizer': opt_name,\n",
        "        'learning_rate': round(lr, 6),\n",
        "        'batch_size': batch_size,\n",
        "        'weight_decay': weight_decay,\n",
        "        'scheduler': scheduler,\n",
        "        'scheduler_params': sample_scheduler_params(scheduler),\n",
        "    }\n",
        "    if opt_name == 'sgd':\n",
        "        trial['momentum'] = random.choice(MOMENTUM_VALUES)\n",
        "    return trial\n",
        "\n",
        "NUM_TRIALS = 30\n",
        "random_trials = [sample_trial() for _ in range(NUM_TRIALS)]\n",
        "\n",
        "for idx, cfg in enumerate(random_trials, start=1):\n",
        "    print(f'Trial {idx}:')\n",
        "    for key, value in cfg.items():\n",
        "        print(f'  {key}: {value}')\n",
        "    print('-' * 30)\n",
        "\n",
        "# Itera random_trials para lanzar entrenamientos y guardar métricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "71fbdc26",
      "metadata": {
        "tags": [
          "training_loop"
        ]
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "EPS = 1e-6\n",
        "\n",
        "class MetricAccumulator:\n",
        "    def __init__(self, num_classes=NUM_CLASSES, eps=EPS):\n",
        "        self.num_classes = num_classes\n",
        "        self.eps = eps\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.intersection = torch.zeros(self.num_classes, dtype=torch.float64)\n",
        "        self.union = torch.zeros(self.num_classes, dtype=torch.float64)\n",
        "        self.pred_sum = torch.zeros(self.num_classes, dtype=torch.float64)\n",
        "        self.target_sum = torch.zeros(self.num_classes, dtype=torch.float64)\n",
        "        self.correct = 0.0\n",
        "        self.total = 0.0\n",
        "\n",
        "    def update(self, logits, targets):\n",
        "        preds = _ensure_multiclass_logits(logits.detach())\n",
        "        pred_labels = torch.argmax(preds, dim=1)\n",
        "        targets = targets.long()\n",
        "        self.correct += (pred_labels == targets).sum().item()\n",
        "        self.total += targets.numel()\n",
        "        for cls in range(self.num_classes):\n",
        "            pred_mask = pred_labels == cls\n",
        "            target_mask = targets == cls\n",
        "            intersection = (pred_mask & target_mask).sum().item()\n",
        "            union = (pred_mask | target_mask).sum().item()\n",
        "            self.intersection[cls] += intersection\n",
        "            self.union[cls] += union\n",
        "            self.pred_sum[cls] += pred_mask.sum().item()\n",
        "            self.target_sum[cls] += target_mask.sum().item()\n",
        "\n",
        "    def export(self):\n",
        "        per_class_iou = {}\n",
        "        per_class_dice = {}\n",
        "        for cls in range(self.num_classes):\n",
        "            if self.union[cls] > 0:\n",
        "                per_class_iou[cls] = (self.intersection[cls] + self.eps) / (self.union[cls] + self.eps)\n",
        "            if (self.pred_sum[cls] + self.target_sum[cls]) > 0:\n",
        "                per_class_dice[cls] = (2 * self.intersection[cls] + self.eps) / (self.pred_sum[cls] + self.target_sum[cls] + self.eps)\n",
        "        mean_iou = sum(per_class_iou.values()) / len(per_class_iou) if per_class_iou else 0.0\n",
        "        mean_dice = sum(per_class_dice.values()) / len(per_class_dice) if per_class_dice else 0.0\n",
        "        acc = self.correct / self.total if self.total else 0.0\n",
        "        return {\n",
        "            'pixel_acc': acc,\n",
        "            'jaccard': {'mean': mean_iou, 'per_class': per_class_iou},\n",
        "            'dice': {'mean': mean_dice, 'per_class': per_class_dice},\n",
        "        }\n",
        "\n",
        "def prepare_targets(mask_tensor):\n",
        "    if mask_tensor.ndim == 4 and mask_tensor.size(1) == 1:\n",
        "        mask_tensor = mask_tensor.squeeze(1)\n",
        "    mask_tensor = (mask_tensor > 0).long()\n",
        "    return mask_tensor\n",
        "\n",
        "def build_optimizer(model, cfg):\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    if cfg['optimizer'] == 'adam':\n",
        "        return torch.optim.Adam(params, lr=cfg['learning_rate'], weight_decay=cfg['weight_decay'])\n",
        "    if cfg['optimizer'] == 'adamw':\n",
        "        return torch.optim.AdamW(params, lr=cfg['learning_rate'], weight_decay=cfg['weight_decay'])\n",
        "    return torch.optim.SGD(\n",
        "        params,\n",
        "        lr=cfg['learning_rate'],\n",
        "        momentum=cfg.get('momentum', 0.9),\n",
        "        weight_decay=cfg['weight_decay'],\n",
        "    )\n",
        "\n",
        "def build_scheduler(optimizer, cfg, total_epochs):\n",
        "    name = cfg.get('scheduler', 'none')\n",
        "    params = cfg.get('scheduler_params', {}) or {}\n",
        "    if name == 'steplr':\n",
        "        return torch.optim.lr_scheduler.StepLR(\n",
        "            optimizer,\n",
        "            step_size=params.get('step_size', 30),\n",
        "            gamma=params.get('gamma', 0.1),\n",
        "        )\n",
        "    if name == 'cosine':\n",
        "        return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=params.get('t_max', total_epochs),\n",
        "        )\n",
        "    if name == 'plateau':\n",
        "        return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='max',\n",
        "            patience=params.get('patience', 4),\n",
        "            factor=params.get('factor', 0.3),\n",
        "        )\n",
        "    return None\n",
        "\n",
        "def prepare_base_state(weights_path, device):\n",
        "    if weights_path is None:\n",
        "        return None\n",
        "    weights_path = Path(weights_path)\n",
        "    if not weights_path.exists():\n",
        "        print(f\"Advertencia: no se encontró {weights_path}\")\n",
        "        return None\n",
        "    checkpoint = torch.load(weights_path, map_location=device)\n",
        "    state_dict = (\n",
        "        checkpoint.get('state_dict')\n",
        "        or checkpoint.get('model_state_dict')\n",
        "        or checkpoint\n",
        "    )\n",
        "    state_dict = dict(state_dict)\n",
        "    state_dict.pop('outc.conv.weight', None)\n",
        "    state_dict.pop('outc.conv.bias', None)\n",
        "    return state_dict\n",
        "\n",
        "def summarize_metrics(split_name, metrics):\n",
        "    dice = metrics['dice']\n",
        "    jacc = metrics['jaccard']\n",
        "    parts = [\n",
        "        f\"{split_name} loss={metrics['loss']:.4f}\",\n",
        "        f\"pixAcc={metrics['pixel_acc']:.4f}\",\n",
        "        f\"dice={dice['mean']:.4f}\",\n",
        "        f\"jacc={jacc['mean']:.4f}\",\n",
        "    ]\n",
        "    for cls, score in sorted(dice['per_class'].items()):\n",
        "        parts.append(f\"dice_c{cls}={score:.4f}\")\n",
        "    for cls, score in sorted(jacc['per_class'].items()):\n",
        "        parts.append(f\"jacc_c{cls}={score:.4f}\")\n",
        "    return ' | '.join(parts)\n",
        "\n",
        "def train_or_eval_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    device,\n",
        "    train_mode=True,\n",
        "    loss_type='cross_entropy',\n",
        "    num_classes=NUM_CLASSES,\n",
        "):\n",
        "    model.train(mode=train_mode)\n",
        "    accumulator = MetricAccumulator(num_classes=num_classes)\n",
        "    running_loss = 0.0\n",
        "    data_count = len(loader.dataset)\n",
        "    with torch.set_grad_enabled(train_mode):\n",
        "        for images, masks in loader:\n",
        "            images = images.to(device)\n",
        "            targets = prepare_targets(masks.to(device))\n",
        "            logits = model(images)\n",
        "            if loss_type == 'bce':\n",
        "                if logits.ndim == 3:\n",
        "                    logits = logits.unsqueeze(1)\n",
        "                target_float = targets.unsqueeze(1).float()\n",
        "                loss = criterion(logits, target_float)\n",
        "            else:\n",
        "                loss = criterion(logits, targets.long())\n",
        "            if train_mode:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            accumulator.update(logits, targets)\n",
        "    metrics = accumulator.export()\n",
        "    metrics['loss'] = running_loss / max(1, data_count)\n",
        "    return metrics\n",
        "\n",
        "def run_single_trial(\n",
        "    cfg,\n",
        "    num_epochs=10,\n",
        "    loss_type='cross_entropy',\n",
        "    base_state_dict=None,\n",
        "    weights_path=None,\n",
        "):\n",
        "    device = get_device()\n",
        "    output_channels = NUM_CLASSES if loss_type != 'bce' else 1\n",
        "    model = ResUNet(n_channels=3, n_classes=output_channels).to(device)\n",
        "    loaded_state = None\n",
        "    if base_state_dict is not None:\n",
        "        loaded_state = base_state_dict\n",
        "    elif weights_path is not None:\n",
        "        loaded_state = prepare_base_state(weights_path, device)\n",
        "    if loaded_state is not None:\n",
        "        model.load_state_dict(loaded_state, strict=False)\n",
        "    criterion = nn.BCEWithLogitsLoss() if loss_type == 'bce' else nn.CrossEntropyLoss()\n",
        "    optimizer = build_optimizer(model, cfg)\n",
        "    scheduler = build_scheduler(optimizer, cfg, total_epochs=num_epochs)\n",
        "    history = []\n",
        "    best_val = -float('inf')\n",
        "    best_state = None\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        start = time.time()\n",
        "        train_metrics = train_or_eval_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            optimizer,\n",
        "            criterion,\n",
        "            device,\n",
        "            train_mode=True,\n",
        "            loss_type=loss_type,\n",
        "            num_classes=NUM_CLASSES,\n",
        "        )\n",
        "        val_metrics = train_or_eval_epoch(\n",
        "            model,\n",
        "            val_loader,\n",
        "            optimizer,\n",
        "            criterion,\n",
        "            device,\n",
        "            train_mode=False,\n",
        "            loss_type=loss_type,\n",
        "            num_classes=NUM_CLASSES,\n",
        "        )\n",
        "        monitor = val_metrics['dice']['mean']\n",
        "        if scheduler:\n",
        "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                scheduler.step(monitor)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "        if monitor > best_val:\n",
        "            best_val = monitor\n",
        "            best_state = deepcopy(model.state_dict())\n",
        "        epoch_time = time.time() - start\n",
        "        history.append(\n",
        "            {\n",
        "                'epoch': epoch,\n",
        "                'train': train_metrics,\n",
        "                'val': val_metrics,\n",
        "                'lr': optimizer.param_groups[0]['lr'],\n",
        "                'time': epoch_time,\n",
        "            }\n",
        "        )\n",
        "        train_summary = summarize_metrics('train', train_metrics)\n",
        "        val_summary = summarize_metrics('val', val_metrics)\n",
        "        print(\n",
        "            f\"[Trial {cfg['optimizer']} | Epoch {epoch}] {train_summary} || {val_summary} || \"\n",
        "            f\"lr={optimizer.param_groups[0]['lr']:.2e}\"\n",
        "        )\n",
        "    return {\n",
        "        'config': cfg,\n",
        "        'history': history,\n",
        "        'best_val_dice': best_val,\n",
        "        'best_state_dict': best_state,\n",
        "    }\n",
        "\n",
        "def run_trials(\n",
        "    trials,\n",
        "    num_epochs=10,\n",
        "    loss_type='cross_entropy',\n",
        "    weights_path=None,\n",
        "    top_k=3,\n",
        "    output_dir=Path('experiments'),\n",
        "):\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    device = get_device()\n",
        "    base_state_dict = prepare_base_state(weights_path, device) if weights_path else None\n",
        "    results = []\n",
        "    top_models = []\n",
        "    for idx, cfg in enumerate(trials, start=1):\n",
        "        print(f'===== Trial {idx}/{len(trials)} =====')\n",
        "        result = run_single_trial(\n",
        "            cfg,\n",
        "            num_epochs=num_epochs,\n",
        "            loss_type=loss_type,\n",
        "            base_state_dict=base_state_dict,\n",
        "        )\n",
        "        results.append(result)\n",
        "        dice_score = result['best_val_dice']\n",
        "        save_path = output_dir / f\"trial_{idx:03d}_dice_{dice_score:.4f}.pt\"\n",
        "        torch.save(\n",
        "            {\n",
        "                'config': result['config'],\n",
        "                'state_dict': result['best_state_dict'],\n",
        "                'history': result['history'],\n",
        "                'best_val_dice': dice_score,\n",
        "            },\n",
        "            save_path,\n",
        "        )\n",
        "        top_models.append({'score': dice_score, 'path': save_path, 'config': result['config']})\n",
        "        top_models.sort(key=lambda item: item['score'], reverse=True)\n",
        "        while len(top_models) > top_k:\n",
        "            removed = top_models.pop()\n",
        "            if removed['path'].exists():\n",
        "                removed['path'].unlink()\n",
        "        print('Top actual:')\n",
        "        for rank, item in enumerate(top_models, start=1):\n",
        "            print(f\"  {rank}) Dice={item['score']:.4f} -> {item['path'].name}\")\n",
        "    return {'results': results, 'top_models': top_models}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "b0ef5041",
      "metadata": {
        "tags": [
          "plotting"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Trial 1/30 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m         plt.tight_layout()\n\u001b[32m     51\u001b[39m         plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m summary = \u001b[43mrun_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcross_entropy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHTS_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m plot_top_models(summary)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 261\u001b[39m, in \u001b[36mrun_trials\u001b[39m\u001b[34m(trials, num_epochs, loss_type, weights_path, top_k, output_dir)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, cfg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trials, start=\u001b[32m1\u001b[39m):\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m===== Trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(trials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m =====\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     result = \u001b[43mrun_single_trial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     results.append(result)\n\u001b[32m    268\u001b[39m     dice_score = result[\u001b[33m'\u001b[39m\u001b[33mbest_val_dice\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 193\u001b[39m, in \u001b[36mrun_single_trial\u001b[39m\u001b[34m(cfg, num_epochs, loss_type, base_state_dict, weights_path)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m    192\u001b[39m     start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     train_metrics = \u001b[43mtrain_or_eval_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m     val_metrics = train_or_eval_epoch(\n\u001b[32m    204\u001b[39m         model,\n\u001b[32m    205\u001b[39m         val_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m    211\u001b[39m         num_classes=NUM_CLASSES,\n\u001b[32m    212\u001b[39m     )\n\u001b[32m    213\u001b[39m     monitor = val_metrics[\u001b[33m'\u001b[39m\u001b[33mdice\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 160\u001b[39m, in \u001b[36mtrain_or_eval_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device, train_mode, loss_type, num_classes)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_mode:\n\u001b[32m    159\u001b[39m     optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     optimizer.step()\n\u001b[32m    162\u001b[39m running_loss += loss.item() * images.size(\u001b[32m0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _extract_metric(history, split, metric, cls=None):\n",
        "    values = []\n",
        "    for record in history:\n",
        "        entry = record[split]\n",
        "        if metric == 'loss':\n",
        "            values.append(entry['loss'])\n",
        "        elif metric == 'pixel_acc':\n",
        "            values.append(entry['pixel_acc'])\n",
        "        elif metric in {'dice', 'jaccard'}:\n",
        "            if cls is None:\n",
        "                values.append(entry[metric]['mean'])\n",
        "            else:\n",
        "                values.append(entry[metric]['per_class'].get(cls, float('nan')))\n",
        "        else:\n",
        "            raise ValueError(f'Métrica desconocida: {metric}')\n",
        "    return values\n",
        "\n",
        "def plot_top_models(summary, metrics=('loss', 'pixel_acc', 'dice', 'jaccard'), splits=('train', 'val')):\n",
        "    if not summary or 'top_models' not in summary:\n",
        "        raise ValueError('Debes proporcionar el resumen retornado por run_trials')\n",
        "    top_models = summary['top_models']\n",
        "    if not top_models:\n",
        "        raise ValueError('No hay modelos en el ranking top-k')\n",
        "    for rank, entry in enumerate(top_models, start=1):\n",
        "        payload = torch.load(entry['path'])\n",
        "        history = payload['history']\n",
        "        epochs = [rec['epoch'] for rec in history]\n",
        "        fig, axes = plt.subplots(len(metrics), 1, figsize=(10, 4 * len(metrics)), sharex=True)\n",
        "        if len(metrics) == 1:\n",
        "            axes = [axes]\n",
        "        fig.suptitle(f\"Top {rank} | Dice={entry['score']:.4f} | {entry['path'].name}\")\n",
        "        for ax, metric in zip(axes, metrics):\n",
        "            for split in splits:\n",
        "                if metric in {'dice', 'jaccard'}:\n",
        "                    mean_values = _extract_metric(history, split, metric)\n",
        "                    ax.plot(epochs, mean_values, label=f\"{split}-{metric}-mean\")\n",
        "                    for cls in range(NUM_CLASSES):\n",
        "                        cls_values = _extract_metric(history, split, metric, cls=cls)\n",
        "                        ax.plot(epochs, cls_values, linestyle='--', alpha=0.6, label=f\"{split}-{metric}-cls{cls}\")\n",
        "                else:\n",
        "                    values = _extract_metric(history, split, metric)\n",
        "                    ax.plot(epochs, values, label=f\"{split}-{metric}\")\n",
        "            ax.set_ylabel(metric)\n",
        "            ax.grid(True)\n",
        "            ax.legend(loc='best')\n",
        "        axes[-1].set_xlabel('Epoch')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "summary = run_trials(random_trials, num_epochs=10, loss_type='cross_entropy', weights_path=WEIGHTS_PATH, top_k=3)\n",
        "plot_top_models(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03735feb",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
