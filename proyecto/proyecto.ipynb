{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b78ff95a",
      "metadata": {},
      "source": [
        "# **Proyecto Final Inteligencia Artificial**\n",
        "\n",
        "### Autores: **Angel David Piñeros Sierra**, **Camilo Andrés Roncancio Toca**, **Kelly Johana Solano Calderón**\n",
        "### Presentado a: **Darwin Eduardo Martinez Riaño**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a410ac",
      "metadata": {},
      "source": [
        "## **Modelo de segmentación de imágenes para la localización de lesiones asociadas al cáncer de piel**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb7b245c",
      "metadata": {},
      "source": [
        "### *Glosario*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7598b777",
      "metadata": {},
      "source": [
        "### *(A) Descripción de la problemática*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a0a7714",
      "metadata": {},
      "source": [
        "### *(B) Objetivo*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca1cd0cb",
      "metadata": {},
      "source": [
        "### *(C) Descripción del dataset*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa109c33",
      "metadata": {},
      "source": [
        "El dataset seleccionado para la evaluación del modelo fue el denominado “Skin cancer: HAM10000” de la plataforma de Kaggle, el cual ofrece un conjunto de imágenes especiales para realizar tareas de segmentación y clasificación. Para el propósito de segmentación, el dataset incluye para cada una de las imágenes, el conjunto de máscaras qué determinan la segmentación de las lesiones de cáncer de piel. \n",
        "\n",
        "> El acrónimo HAM10000 significa “Human Against Machine with 10000 training images”. \n",
        "\n",
        "Este dataset es una recopilación de imágenes demoscópicas de diferentes poblaciones. Estas fueron originalmente publicadas inicialmente en el repositorio de Harvard Dataverse,  con el propósito de abordar la dificultad de encontrar un dataset lo suficientemente grande y diverso para realizar diagnósticos automatizados de lesiones cutáneas pigmentadas. \n",
        "\n",
        "El dataset se conforma de dos carpetas: images y masks. Cada una con **10015** imágenes en formato **.JPEG**. Todas las imágenes tienen una dimensión de `600px X 450px`\n",
        "\n",
        "<img src=\"https://res.cloudinary.com/dlsntlruu/image/upload/v1764556079/carpeta_images_pieoyu.png\" width=\"600px\"/>\n",
        "\n",
        "<img src=\"https://res.cloudinary.com/dlsntlruu/image/upload/v1764556079/carpeta_masks_taifwn.png\" width=\"600px\"/>\n",
        "\n",
        "Las imágenes incluyen diagnósticos de:\n",
        "*  Queratosis actínicas\n",
        "*  Carcinoma intraepitelial\n",
        "*  Carcinoma basocelular\n",
        "*  Lesiones de tipo queratosis\n",
        "*  Dermatofibroma\n",
        "*  Melanoma\n",
        "*  Lesiones vasculares\n",
        "\n",
        "Contar con una amplia gama de diagnósticos permite qué la tarea de segmentación semántica pueda realizarse de forma óptima. \n",
        "\n",
        "Para mayor información: \n",
        "\n",
        "*  Skin cancer: HAM10000: https://www.kaggle.com/datasets/surajghuwalewala/ham1000-segmentation-and-classification/\n",
        "*  The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "313944b8",
      "metadata": {},
      "source": [
        "### *(D) Importación y organización de datos*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "558cb2b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (1.8.2)\n",
            "Requirement already satisfied: pandas in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (2.3.3)\n",
            "Requirement already satisfied: torch in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (2.9.1)\n",
            "Requirement already satisfied: PILlow in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (12.0.0)\n",
            "Requirement already satisfied: torchvision in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (0.24.1)\n",
            "Requirement already satisfied: black>=24.10.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (25.11.0)\n",
            "Requirement already satisfied: bleach in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: kagglesdk in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (0.1.13)\n",
            "Requirement already satisfied: mypy>=1.15.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (1.19.0)\n",
            "Requirement already satisfied: protobuf in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (6.33.1)\n",
            "Requirement already satisfied: python-dateutil in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.32.5)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (80.9.0)\n",
            "Requirement already satisfied: six>=1.10 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: types-requests in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.32.4.20250913)\n",
            "Requirement already satisfied: types-tqdm in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (4.67.0.20250809)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from pandas) (2.3.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.5.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (8.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (1.1.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (25.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (4.5.0)\n",
            "Requirement already satisfied: pytokens>=0.3.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (0.3.0)\n",
            "Requirement already satisfied: librt>=0.6.2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from mypy>=1.15.0->kaggle) (0.6.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: webencodings in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from requests->kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from requests->kaggle) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from requests->kaggle) (2025.11.12)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle pandas torch PILlow torchvision "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "453f4670",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.v2 as v2\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "c8077e39",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La carpeta 'ham1000_data' ya existe. No se descargará de nuevo.\n",
            "Contenido de la carpeta: ['GroundTruth.csv', 'images', 'masks']\n",
            "Número de imágenes: 10017\n",
            "Número de máscaras: 10015\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "dataset_url = \"surajghuwalewala/ham1000-segmentation-and-classification\"\n",
        "ruta_descarga = \"ham1000_data\"\n",
        "\n",
        "if os.path.exists(ruta_descarga):\n",
        "    print(f\"La carpeta '{ruta_descarga}' ya existe. No se descargará de nuevo.\")\n",
        "else:\n",
        "    os.makedirs(ruta_descarga, exist_ok=True)\n",
        "    subprocess.run([\n",
        "        \"kaggle\", \"datasets\", \"download\",\n",
        "        \"-d\", dataset_url,\n",
        "        \"-p\", ruta_descarga,\n",
        "        \"--unzip\"\n",
        "    ], check=True)\n",
        "\n",
        "print(\"Contenido de la carpeta:\", os.listdir(ruta_descarga))\n",
        "\n",
        "images_dir = os.path.join(ruta_descarga, \"images\")\n",
        "masks_dir = os.path.join(ruta_descarga, \"masks\")\n",
        "\n",
        "if os.path.isdir(images_dir):\n",
        "    print(\"Número de imágenes:\", len(os.listdir(images_dir)))\n",
        "else:\n",
        "    print(\"No encontré la carpeta 'images'.\")\n",
        "\n",
        "if os.path.isdir(masks_dir):\n",
        "    print(\"Número de máscaras:\", len(os.listdir(masks_dir)))\n",
        "else:\n",
        "    print(\"No encontré la carpeta 'masks'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c09ae392",
      "metadata": {
        "tags": [
          "splits"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 7236 imágenes\n",
            "val: 1276 imágenes\n",
            "test: 1503 imágenes\n",
            "Splits creados en ham1000_data_splits\n"
          ]
        }
      ],
      "source": [
        "DATA_ROOT = Path('ham1000_data')\n",
        "OUTPUT_ROOT = Path('ham1000_data_splits')\n",
        "TRAIN_RATIO = 0.85  # train + val\n",
        "VAL_FROM_TRAIN = 0.15  # porcentaje de train destinado a validación\n",
        "SEED = 42\n",
        "MASK_SUFFIX = '_segmentation.png'\n",
        "\n",
        "image_dir = DATA_ROOT / 'images'\n",
        "mask_dir = DATA_ROOT / 'masks'\n",
        "if not image_dir.exists() or not mask_dir.exists():\n",
        "    raise RuntimeError(f\"No se hallaron carpetas esperadas en {DATA_ROOT}.\")\n",
        "\n",
        "random.seed(SEED)\n",
        "if OUTPUT_ROOT.exists():\n",
        "    shutil.rmtree(OUTPUT_ROOT)\n",
        "for split in ('train', 'val', 'test'):\n",
        "    (OUTPUT_ROOT / split / 'images').mkdir(parents=True, exist_ok=True)\n",
        "    (OUTPUT_ROOT / split / 'masks').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "allowed_exts = {'.png', '.jpg', '.jpeg'}\n",
        "image_paths = [p for p in image_dir.iterdir() if p.suffix.lower() in allowed_exts]\n",
        "if not image_paths:\n",
        "    raise RuntimeError(f'No se encontraron imágenes en {image_dir}.')\n",
        "image_paths.sort()\n",
        "random.shuffle(image_paths)\n",
        "\n",
        "total = len(image_paths)\n",
        "train_val_count = int(total * TRAIN_RATIO)\n",
        "val_count = int(train_val_count * VAL_FROM_TRAIN)\n",
        "train_count = train_val_count - val_count\n",
        "\n",
        "splits = {\n",
        "    'train': image_paths[:train_count],\n",
        "    'val': image_paths[train_count: train_count + val_count],\n",
        "    'test': image_paths[train_count + val_count:]\n",
        "}\n",
        "\n",
        "for split_name, files in splits.items():\n",
        "    dst_img = OUTPUT_ROOT / split_name / 'images'\n",
        "    dst_mask = OUTPUT_ROOT / split_name / 'masks'\n",
        "    for img_path in files:\n",
        "        mask_path = mask_dir / f\"{img_path.stem}{MASK_SUFFIX}\"\n",
        "        if not mask_path.exists():\n",
        "            raise FileNotFoundError(f'Falta máscara: {mask_path}')\n",
        "        shutil.copy2(img_path, dst_img / img_path.name)\n",
        "        shutil.copy2(mask_path, dst_mask / mask_path.name)\n",
        "    print(f\"{split_name}: {len(files)} imágenes\")\n",
        "print('Splits creados en', OUTPUT_ROOT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e979a2",
      "metadata": {
        "tags": [
          "train_stats"
        ]
      },
      "outputs": [],
      "source": [
        "TRAIN_IMG_DIR = Path('ham1000_data_splits/train/images')\n",
        "if not TRAIN_IMG_DIR.exists():\n",
        "    raise RuntimeError(f'No existe el directorio: {TRAIN_IMG_DIR}')\n",
        "\n",
        "TARGET_SIZE = (256, 256)\n",
        "resize_transform = transforms.Resize(TARGET_SIZE, interpolation=InterpolationMode.BILINEAR)\n",
        "\n",
        "img_files = sorted(\n",
        "    [p for p in TRAIN_IMG_DIR.iterdir() if p.suffix.lower() in {'.png', '.jpg', '.jpeg'}]\n",
        ")\n",
        "if not img_files:\n",
        "    raise RuntimeError(f'No se encontraron imágenes en {TRAIN_IMG_DIR}')\n",
        "\n",
        "means, stds = [], []\n",
        "for img_path in img_files:\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_tensor = transforms.ToTensor()(resize_transform(img))\n",
        "    means.append(img_tensor.mean(dim=(1, 2)))\n",
        "    stds.append(img_tensor.std(dim=(1, 2)))\n",
        "\n",
        "IMG_MEAN = torch.stack(means).mean(dim=0)\n",
        "IMG_STD = torch.stack(stds).mean(dim=0)\n",
        "print(f'Media por canal (reescala {TARGET_SIZE[0]}x{TARGET_SIZE[1]}): {IMG_MEAN.tolist()}')\n",
        "print(f'Desviación estándar por canal: {IMG_STD.tolist()}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0461c7f5",
      "metadata": {
        "tags": [
          "transforms"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformaciones definidas:\n",
            "train_joint_transform: Compose(\n",
            "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
            "      RandomHorizontalFlip(p=0.5)\n",
            "      RandomVerticalFlip(p=0.2)\n",
            "      RandomRotation(degrees=[-180.0, 180.0], interpolation=InterpolationMode.NEAREST, expand=False, fill=0)\n",
            "      ElasticTransform(alpha=[40.0, 40.0], sigma=[5.0, 5.0], interpolation=InterpolationMode.BILINEAR, fill=0)\n",
            "      ColorJitter(brightness=(0.85, 1.15), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.02, 0.02))\n",
            "      GaussianBlur(kernel_size=(3, 3), sigma=[0.1, 1.0])\n",
            "      ToDtype(scale=True)\n",
            "      Normalize(mean=[tensor(0.7633), tensor(0.5455), tensor(0.5698)], std=[tensor(0.0887), tensor(0.1173), tensor(0.1318)], inplace=False)\n",
            ")\n",
            "val_test_joint_transform: Compose(\n",
            "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
            "      ToDtype(scale=True)\n",
            "      Normalize(mean=[tensor(0.7633), tensor(0.5455), tensor(0.5698)], std=[tensor(0.0887), tensor(0.1173), tensor(0.1318)], inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "normalize_transform = v2.Normalize(mean=IMG_MEAN, std=IMG_STD)\n",
        "\n",
        "train_joint_transform = v2.Compose([\n",
        "    v2.Resize(TARGET_SIZE, interpolation=InterpolationMode.BILINEAR, antialias=True),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.RandomVerticalFlip(p=0.2),\n",
        "    v2.RandomRotation(degrees=(-180, 180)),\n",
        "    v2.ElasticTransform(alpha=40.0, sigma=5.0, interpolation=InterpolationMode.BILINEAR),\n",
        "    v2.ColorJitter(brightness=0.15, contrast=0.2, saturation=0.2, hue=0.02),\n",
        "    v2.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    normalize_transform,\n",
        "])\n",
        "\n",
        "val_test_joint_transform = v2.Compose([\n",
        "    v2.Resize(TARGET_SIZE, interpolation=InterpolationMode.BILINEAR, antialias=True),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    normalize_transform,\n",
        "])\n",
        "\n",
        "def apply_train_transforms(image, mask):\n",
        "    image_tv = tv_tensors.Image(image)\n",
        "    mask_tv = tv_tensors.Mask(mask)\n",
        "    image_aug, mask_aug = train_joint_transform(image_tv, mask_tv)\n",
        "    return image_aug, mask_aug\n",
        "\n",
        "def apply_val_transforms(image, mask):\n",
        "    image_tv = tv_tensors.Image(image)\n",
        "    mask_tv = tv_tensors.Mask(mask)\n",
        "    image_val, mask_val = val_test_joint_transform(image_tv, mask_tv)\n",
        "    return image_val, mask_val\n",
        "\n",
        "def apply_test_transforms(image, mask):\n",
        "    image_tv = tv_tensors.Image(image)\n",
        "    mask_tv = tv_tensors.Mask(mask)\n",
        "    image_test, mask_test = val_test_joint_transform(image_tv, mask_tv)\n",
        "    return image_test, mask_test\n",
        "\n",
        "print(\"Transformaciones definidas:\")\n",
        "print(\"train_joint_transform:\", train_joint_transform)\n",
        "print(\"val_test_joint_transform:\", val_test_joint_transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c73fa70f",
      "metadata": {
        "tags": [
          "dataset"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 7236 muestras\n",
            "val: 1276 muestras\n",
            "test: 1503 muestras\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class HAM1000SegmentationDataset(Dataset):\n",
        "    def __init__(self, root_dir: Path, split: str, transform_fn=None, mask_suffix: str = '_segmentation.png'):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        if split not in {'train', 'val', 'test'}:\n",
        "            raise ValueError(\"split debe ser 'train', 'val' o 'test'\")\n",
        "        self.split = split\n",
        "        self.images_dir = self.root_dir / split / 'images'\n",
        "        self.masks_dir = self.root_dir / split / 'masks'\n",
        "        if not self.images_dir.exists() or not self.masks_dir.exists():\n",
        "            raise RuntimeError(f'No se hallan carpetas para el split {split} en {self.root_dir}')\n",
        "        self.mask_suffix = mask_suffix\n",
        "        self.transform_fn = transform_fn\n",
        "\n",
        "        allowed_exts = {'.png', '.jpg', '.jpeg'}\n",
        "        self.samples = []\n",
        "        for img_path in sorted(self.images_dir.iterdir()):\n",
        "            if img_path.suffix.lower() not in allowed_exts:\n",
        "                continue\n",
        "            mask_path = self.masks_dir / f\"{img_path.stem}{self.mask_suffix}\"\n",
        "            if not mask_path.exists():\n",
        "                continue\n",
        "            self.samples.append((img_path, mask_path))\n",
        "        if not self.samples:\n",
        "            raise RuntimeError(f'No se encontraron pares imagen-máscara en {self.images_dir}')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        if self.transform_fn is not None:\n",
        "            image, mask = self.transform_fn(image, mask)\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "def get_transform_fn(split: str):\n",
        "    if split == 'train':\n",
        "        return apply_train_transforms\n",
        "    if split == 'val':\n",
        "        return apply_val_transforms\n",
        "    if split == 'test':\n",
        "        return apply_test_transforms\n",
        "    raise ValueError('split desconocido')\n",
        "\n",
        "\n",
        "def create_datasets(root_dir: Path = Path('ham1000_data_splits')):\n",
        "    datasets = {}\n",
        "    for split in ('train', 'val', 'test'):\n",
        "        datasets[split] = HAM1000SegmentationDataset(\n",
        "            root_dir=root_dir,\n",
        "            split=split,\n",
        "            transform_fn=get_transform_fn(split),\n",
        "        )\n",
        "        print(f\"{split}: {len(datasets[split])} muestras\")\n",
        "    return datasets\n",
        "\n",
        "ham_datasets = create_datasets()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add8745d",
      "metadata": {
        "tags": [
          "dataloader"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaders listos:\n",
            "train: 905 batches\n",
            "val: 160 batches\n",
            "test: 188 batches\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 4\n",
        "PIN_MEMORY = True\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ham_datasets['train'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    ham_datasets['val'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    ham_datasets['test'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        ")\n",
        "\n",
        "print('Loaders listos:')\n",
        "print(f\"train: {len(train_loader)} batches\")\n",
        "print(f\"val: {len(val_loader)} batches\")\n",
        "print(f\"test: {len(test_loader)} batches\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4873482",
      "metadata": {
        "tags": [
          "model"
        ]
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Two Conv-BN blocks with a residual shortcut.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = out + identity\n",
        "        return F.relu(out)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"MaxPool + ResidualBlock encoder stage.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.resblock = ResidualBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(x)\n",
        "        x = self.resblock(x)\n",
        "        return x\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"ConvTranspose2d upsampling + concat + ResidualBlock.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.resblock = ResidualBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diff_y = x2.size(2) - x1.size(2)\n",
        "        diff_x = x2.size(3) - x1.size(3)\n",
        "        if diff_y != 0 or diff_x != 0:\n",
        "            x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2, diff_y // 2, diff_y - diff_y // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.resblock(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class ResUNet(nn.Module):\n",
        "    def __init__(self, n_channels=3, n_classes=2):\n",
        "        super().__init__()\n",
        "        self.inc = ResidualBlock(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        return self.outc(x)\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5434279d",
      "metadata": {
        "tags": [
          "metrics"
        ]
      },
      "outputs": [],
      "source": [
        "def _ensure_multiclass_logits(preds):\n",
        "    if preds.ndim != 4:\n",
        "        raise ValueError('preds debe tener forma [B, C, H, W]')\n",
        "    if preds.shape[1] == 1:\n",
        "        probs = torch.sigmoid(preds)\n",
        "        return torch.cat([1.0 - probs, probs], dim=1)\n",
        "    return preds\n",
        "\n",
        "def pixel_accuracy(preds, targets):\n",
        "    preds = _ensure_multiclass_logits(preds)\n",
        "    pred_labels = torch.argmax(preds, dim=1)\n",
        "    correct = (pred_labels == targets).float().sum()\n",
        "    total = torch.numel(targets)\n",
        "    return (correct / total).item()\n",
        "\n",
        "def _per_class_iou(pred_labels, targets, num_classes=2, eps=1e-6):\n",
        "    per_class = {}\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = pred_labels == cls\n",
        "        target_cls = targets == cls\n",
        "        intersection = (pred_cls & target_cls).float().sum()\n",
        "        union = (pred_cls | target_cls).float().sum()\n",
        "        if union == 0:\n",
        "            continue\n",
        "        per_class[cls] = ((intersection + eps) / (union + eps)).item()\n",
        "    return per_class\n",
        "\n",
        "def _per_class_dice(pred_labels, targets, num_classes=2, eps=1e-6):\n",
        "    per_class = {}\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = pred_labels == cls\n",
        "        target_cls = targets == cls\n",
        "        intersection = (pred_cls & target_cls).float().sum()\n",
        "        pred_sum = pred_cls.float().sum()\n",
        "        target_sum = target_cls.float().sum()\n",
        "        denom = pred_sum + target_sum\n",
        "        if denom == 0:\n",
        "            continue\n",
        "        per_class[cls] = ((2 * intersection + eps) / (denom + eps)).item()\n",
        "    return per_class\n",
        "\n",
        "def compute_jaccard(preds, targets, num_classes=2, eps=1e-6):\n",
        "    preds = _ensure_multiclass_logits(preds)\n",
        "    pred_labels = torch.argmax(preds, dim=1)\n",
        "    per_class = _per_class_iou(pred_labels, targets, num_classes, eps)\n",
        "    if not per_class:\n",
        "        return {'mean': 0.0, 'per_class': per_class}\n",
        "    mean_score = sum(per_class.values()) / len(per_class)\n",
        "    return {'mean': mean_score, 'per_class': per_class}\n",
        "\n",
        "def compute_dice(preds, targets, num_classes=2, eps=1e-6):\n",
        "    preds = _ensure_multiclass_logits(preds)\n",
        "    pred_labels = torch.argmax(preds, dim=1)\n",
        "    per_class = _per_class_dice(pred_labels, targets, num_classes, eps)\n",
        "    if not per_class:\n",
        "        return {'mean': 0.0, 'per_class': per_class}\n",
        "    mean_score = sum(per_class.values()) / len(per_class)\n",
        "    return {'mean': mean_score, 'per_class': per_class}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5be89b9",
      "metadata": {
        "tags": [
          "model_load"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo cargado desde resunet_gsd_lr5e4.pt\n",
            "Dispositivo: cpu\n",
            "Keys ignoradas: ['outc.conv.weight', 'outc.conv.bias']\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "WEIGHTS_PATH = Path('resunet_gsd_lr5e4.pt')\n",
        "device = get_device()\n",
        "model = ResUNet().to(device)\n",
        "\n",
        "checkpoint = torch.load(WEIGHTS_PATH, map_location=device)\n",
        "state_dict = (\n",
        "    checkpoint.get('state_dict')\n",
        "    or checkpoint.get('model_state_dict')\n",
        "    or checkpoint\n",
        ")\n",
        "for key in ('outc.conv.weight', 'outc.conv.bias'):\n",
        "    state_dict.pop(key, None)\n",
        "missing = model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "print(f'Modelo cargado desde {WEIGHTS_PATH}')\n",
        "print('Dispositivo:', device)\n",
        "print('Keys ignoradas:', missing.missing_keys)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc3343b2",
      "metadata": {
        "tags": [
          "hparam_search"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000163\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "------------------------------\n",
            "Trial 2:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000103\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 20}\n",
            "------------------------------\n",
            "Trial 3:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000112\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "------------------------------\n",
            "Trial 4:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000214\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.1}\n",
            "------------------------------\n",
            "Trial 5:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.001675\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0\n",
            "  scheduler: steplr\n",
            "  scheduler_params: {'step_size': 30, 'gamma': 0.1}\n",
            "------------------------------\n",
            "Trial 6:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000133\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "------------------------------\n",
            "Trial 7:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000912\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 20}\n",
            "------------------------------\n",
            "Trial 8:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000215\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "------------------------------\n",
            "Trial 9:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.00063\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 20}\n",
            "------------------------------\n",
            "Trial 10:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000115\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.1}\n",
            "------------------------------\n",
            "Trial 11:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.001599\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "------------------------------\n",
            "Trial 12:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 5.2e-05\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 20}\n",
            "------------------------------\n",
            "Trial 13:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000325\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "------------------------------\n",
            "Trial 14:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000201\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0\n",
            "  scheduler: steplr\n",
            "  scheduler_params: {'step_size': 15, 'gamma': 0.1}\n",
            "------------------------------\n",
            "Trial 15:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000396\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "------------------------------\n",
            "Trial 16:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000164\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "------------------------------\n",
            "Trial 17:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000136\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.3}\n",
            "------------------------------\n",
            "Trial 18:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.040251\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 5, 'factor': 0.1}\n",
            "  momentum: 0.99\n",
            "------------------------------\n",
            "Trial 19:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000629\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "------------------------------\n",
            "Trial 20:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000656\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.1}\n",
            "------------------------------\n",
            "Trial 21:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000143\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.1}\n",
            "------------------------------\n",
            "Trial 22:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.024266\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.3}\n",
            "  momentum: 0.9\n",
            "------------------------------\n",
            "Trial 23:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000122\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "------------------------------\n",
            "Trial 24:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000925\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.1}\n",
            "------------------------------\n",
            "Trial 25:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.039404\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "  momentum: 0.99\n",
            "------------------------------\n",
            "Trial 26:\n",
            "  optimizer: adam\n",
            "  learning_rate: 0.000182\n",
            "  batch_size: 12\n",
            "  weight_decay: 0.0\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 20}\n",
            "------------------------------\n",
            "Trial 27:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.009168\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0001\n",
            "  scheduler: none\n",
            "  scheduler_params: {}\n",
            "  momentum: 0.99\n",
            "------------------------------\n",
            "Trial 28:\n",
            "  optimizer: adamw\n",
            "  learning_rate: 0.000745\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0\n",
            "  scheduler: plateau\n",
            "  scheduler_params: {'patience': 3, 'factor': 0.3}\n",
            "------------------------------\n",
            "Trial 29:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.018989\n",
            "  batch_size: 8\n",
            "  weight_decay: 0.0\n",
            "  scheduler: steplr\n",
            "  scheduler_params: {'step_size': 30, 'gamma': 0.1}\n",
            "  momentum: 0.9\n",
            "------------------------------\n",
            "Trial 30:\n",
            "  optimizer: sgd\n",
            "  learning_rate: 0.018748\n",
            "  batch_size: 16\n",
            "  weight_decay: 0.0005\n",
            "  scheduler: cosine\n",
            "  scheduler_params: {'t_max': 40}\n",
            "  momentum: 0.9\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "random.seed(45)\n",
        "\n",
        "OPTIMIZERS = ('adam', 'adamw', 'sgd')\n",
        "BATCH_CHOICES = (8, 12, 16)\n",
        "WEIGHT_DECAY_VALUES = (0.0, 1e-4, 5e-4)\n",
        "MOMENTUM_VALUES = (0.85, 0.9, 0.95, 0.99)\n",
        "SCHEDULERS = {\n",
        "    'none': {},\n",
        "    'steplr': {'step_size': (15, 30), 'gamma': (0.5, 0.1)},\n",
        "    'cosine': {'t_max': (20, 40)},\n",
        "    'plateau': {'patience': (3, 5), 'factor': (0.3, 0.1)},\n",
        "}\n",
        "\n",
        "def sample_lr(opt_name):\n",
        "    if opt_name == 'sgd':\n",
        "        return 10 ** random.uniform(-2.3, -1.5)\n",
        "    if opt_name == 'adamw':\n",
        "        return 10 ** random.uniform(-4.3, -3.0)\n",
        "    return 10 ** random.uniform(-4.0, -3.0)\n",
        "\n",
        "def sample_scheduler_params(name):\n",
        "    params = {}\n",
        "    cfg = SCHEDULERS[name]\n",
        "    for key, bounds in cfg.items():\n",
        "        params[key] = random.choice(bounds)\n",
        "    return params\n",
        "\n",
        "def sample_trial():\n",
        "    opt_name = random.choice(OPTIMIZERS)\n",
        "    batch_size = random.choice(BATCH_CHOICES)\n",
        "    weight_decay = random.choice(WEIGHT_DECAY_VALUES)\n",
        "    lr = sample_lr(opt_name) * (batch_size / 8)\n",
        "    scheduler = random.choice(tuple(SCHEDULERS.keys()))\n",
        "    trial = {\n",
        "        'optimizer': opt_name,\n",
        "        'learning_rate': round(lr, 6),\n",
        "        'batch_size': batch_size,\n",
        "        'weight_decay': weight_decay,\n",
        "        'scheduler': scheduler,\n",
        "        'scheduler_params': sample_scheduler_params(scheduler),\n",
        "    }\n",
        "    if opt_name == 'sgd':\n",
        "        trial['momentum'] = random.choice(MOMENTUM_VALUES)\n",
        "    return trial\n",
        "\n",
        "NUM_TRIALS = 30\n",
        "random_trials = [sample_trial() for _ in range(NUM_TRIALS)]\n",
        "\n",
        "for idx, cfg in enumerate(random_trials, start=1):\n",
        "    print(f'Trial {idx}:')\n",
        "    for key, value in cfg.items():\n",
        "        print(f'  {key}: {value}')\n",
        "    print('-' * 30)\n",
        "\n",
        "# Itera random_trials para lanzar entrenamientos y guardar métricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71fbdc26",
      "metadata": {
        "tags": [
          "training_loop"
        ]
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "EPS = 1e-6\n",
        "\n",
        "class MetricAccumulator:\n",
        "    def __init__(self, num_classes=NUM_CLASSES, eps=EPS):\n",
        "        self.num_classes = num_classes\n",
        "        self.eps = eps\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.intersection = torch.zeros(self.num_classes, dtype=torch.float64)\n",
        "        self.union = torch.zeros(self.num_classes, dtype=torch.float64)\n",
        "        self.pred_sum = torch.zeros(self.num_classes, dtype=torch.float64)\n",
        "        self.target_sum = torch.zeros(self.num_classes, dtype=torch.float64)\n",
        "        self.correct = 0.0\n",
        "        self.total = 0.0\n",
        "\n",
        "    def update(self, logits, targets):\n",
        "        preds = _ensure_multiclass_logits(logits.detach())\n",
        "        pred_labels = torch.argmax(preds, dim=1)\n",
        "        targets = targets.long()\n",
        "        self.correct += (pred_labels == targets).sum().item()\n",
        "        self.total += targets.numel()\n",
        "        for cls in range(self.num_classes):\n",
        "            pred_mask = pred_labels == cls\n",
        "            target_mask = targets == cls\n",
        "            intersection = (pred_mask & target_mask).sum().item()\n",
        "            union = (pred_mask | target_mask).sum().item()\n",
        "            self.intersection[cls] += intersection\n",
        "            self.union[cls] += union\n",
        "            self.pred_sum[cls] += pred_mask.sum().item()\n",
        "            self.target_sum[cls] += target_mask.sum().item()\n",
        "\n",
        "    def export(self):\n",
        "        per_class_iou = {}\n",
        "        per_class_dice = {}\n",
        "        for cls in range(self.num_classes):\n",
        "            if self.union[cls] > 0:\n",
        "                per_class_iou[cls] = (self.intersection[cls] + self.eps) / (self.union[cls] + self.eps)\n",
        "            if (self.pred_sum[cls] + self.target_sum[cls]) > 0:\n",
        "                per_class_dice[cls] = (2 * self.intersection[cls] + self.eps) / (self.pred_sum[cls] + self.target_sum[cls] + self.eps)\n",
        "        mean_iou = sum(per_class_iou.values()) / len(per_class_iou) if per_class_iou else 0.0\n",
        "        mean_dice = sum(per_class_dice.values()) / len(per_class_dice) if per_class_dice else 0.0\n",
        "        acc = self.correct / self.total if self.total else 0.0\n",
        "        return {\n",
        "            'pixel_acc': acc,\n",
        "            'jaccard': {'mean': mean_iou, 'per_class': per_class_iou},\n",
        "            'dice': {'mean': mean_dice, 'per_class': per_class_dice},\n",
        "        }\n",
        "\n",
        "def prepare_targets(mask_tensor):\n",
        "    if mask_tensor.ndim == 4 and mask_tensor.size(1) == 1:\n",
        "        mask_tensor = mask_tensor.squeeze(1)\n",
        "    mask_tensor = (mask_tensor > 0).long()\n",
        "    return mask_tensor\n",
        "\n",
        "def build_optimizer(model, cfg):\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    if cfg['optimizer'] == 'adam':\n",
        "        return torch.optim.Adam(params, lr=cfg['learning_rate'], weight_decay=cfg['weight_decay'])\n",
        "    if cfg['optimizer'] == 'adamw':\n",
        "        return torch.optim.AdamW(params, lr=cfg['learning_rate'], weight_decay=cfg['weight_decay'])\n",
        "    return torch.optim.SGD(\n",
        "        params,\n",
        "        lr=cfg['learning_rate'],\n",
        "        momentum=cfg.get('momentum', 0.9),\n",
        "        weight_decay=cfg['weight_decay'],\n",
        "    )\n",
        "\n",
        "def build_scheduler(optimizer, cfg, total_epochs):\n",
        "    name = cfg.get('scheduler', 'none')\n",
        "    params = cfg.get('scheduler_params', {}) or {}\n",
        "    if name == 'steplr':\n",
        "        return torch.optim.lr_scheduler.StepLR(\n",
        "            optimizer,\n",
        "            step_size=params.get('step_size', 30),\n",
        "            gamma=params.get('gamma', 0.1),\n",
        "        )\n",
        "    if name == 'cosine':\n",
        "        return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=params.get('t_max', total_epochs),\n",
        "        )\n",
        "    if name == 'plateau':\n",
        "        return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='max',\n",
        "            patience=params.get('patience', 4),\n",
        "            factor=params.get('factor', 0.3),\n",
        "        )\n",
        "    return None\n",
        "\n",
        "def prepare_base_state(weights_path, device):\n",
        "    if weights_path is None:\n",
        "        return None\n",
        "    weights_path = Path(weights_path)\n",
        "    if not weights_path.exists():\n",
        "        print(f\"Advertencia: no se encontró {weights_path}\")\n",
        "        return None\n",
        "    checkpoint = torch.load(weights_path, map_location=device)\n",
        "    state_dict = (\n",
        "        checkpoint.get('state_dict')\n",
        "        or checkpoint.get('model_state_dict')\n",
        "        or checkpoint\n",
        "    )\n",
        "    state_dict = dict(state_dict)\n",
        "    state_dict.pop('outc.conv.weight', None)\n",
        "    state_dict.pop('outc.conv.bias', None)\n",
        "    return state_dict\n",
        "\n",
        "def train_or_eval_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    device,\n",
        "    train_mode=True,\n",
        "    loss_type='cross_entropy',\n",
        "    num_classes=NUM_CLASSES,\n",
        "):\n",
        "    model.train(mode=train_mode)\n",
        "    accumulator = MetricAccumulator(num_classes=num_classes)\n",
        "    running_loss = 0.0\n",
        "    data_count = len(loader.dataset)\n",
        "    with torch.set_grad_enabled(train_mode):\n",
        "        for images, masks in loader:\n",
        "            images = images.to(device)\n",
        "            targets = prepare_targets(masks.to(device))\n",
        "            logits = model(images)\n",
        "            if loss_type == 'bce':\n",
        "                if logits.ndim == 3:\n",
        "                    logits = logits.unsqueeze(1)\n",
        "                target_float = targets.unsqueeze(1).float()\n",
        "                loss = criterion(logits, target_float)\n",
        "            else:\n",
        "                loss = criterion(logits, targets.long())\n",
        "            if train_mode:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            accumulator.update(logits, targets)\n",
        "    metrics = accumulator.export()\n",
        "    metrics['loss'] = running_loss / max(1, data_count)\n",
        "    return metrics\n",
        "\n",
        "def run_single_trial(\n",
        "    cfg,\n",
        "    num_epochs=10,\n",
        "    loss_type='cross_entropy',\n",
        "    base_state_dict=None,\n",
        "    weights_path=None,\n",
        "):\n",
        "    device = get_device()\n",
        "    output_channels = NUM_CLASSES if loss_type != 'bce' else 1\n",
        "    model = ResUNet(n_channels=3, n_classes=output_channels).to(device)\n",
        "    loaded_state = None\n",
        "    if base_state_dict is not None:\n",
        "        loaded_state = base_state_dict\n",
        "    elif weights_path is not None:\n",
        "        loaded_state = prepare_base_state(weights_path, device)\n",
        "    if loaded_state is not None:\n",
        "        model.load_state_dict(loaded_state, strict=False)\n",
        "    criterion = nn.BCEWithLogitsLoss() if loss_type == 'bce' else nn.CrossEntropyLoss()\n",
        "    optimizer = build_optimizer(model, cfg)\n",
        "    scheduler = build_scheduler(optimizer, cfg, total_epochs=num_epochs)\n",
        "    history = []\n",
        "    best_val = -float('inf')\n",
        "    best_state = None\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        start = time.time()\n",
        "        train_metrics = train_or_eval_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            optimizer,\n",
        "            criterion,\n",
        "            device,\n",
        "            train_mode=True,\n",
        "            loss_type=loss_type,\n",
        "            num_classes=NUM_CLASSES,\n",
        "        )\n",
        "        val_metrics = train_or_eval_epoch(\n",
        "            model,\n",
        "            val_loader,\n",
        "            optimizer,\n",
        "            criterion,\n",
        "            device,\n",
        "            train_mode=False,\n",
        "            loss_type=loss_type,\n",
        "            num_classes=NUM_CLASSES,\n",
        "        )\n",
        "        monitor = val_metrics['dice']['mean']\n",
        "        if scheduler:\n",
        "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                scheduler.step(monitor)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "        if monitor > best_val:\n",
        "            best_val = monitor\n",
        "            best_state = deepcopy(model.state_dict())\n",
        "        epoch_time = time.time() - start\n",
        "        history.append(\n",
        "            {\n",
        "                'epoch': epoch,\n",
        "                'train': train_metrics,\n",
        "                'val': val_metrics,\n",
        "                'lr': optimizer.param_groups[0]['lr'],\n",
        "                'time': epoch_time,\n",
        "            }\n",
        "        )\n",
        "        print(\n",
        "            f\"[Trial {cfg['optimizer']} | Epoch {epoch}] train Dice={train_metrics['dice']['mean']:.4f} | \"\n",
        "            f\"val Dice={val_metrics['dice']['mean']:.4f} | lr={optimizer.param_groups[0]['lr']:.2e}\"\n",
        "        )\n",
        "    return {\n",
        "        'config': cfg,\n",
        "        'history': history,\n",
        "        'best_val_dice': best_val,\n",
        "        'best_state_dict': best_state,\n",
        "    }\n",
        "\n",
        "def run_trials(\n",
        "    trials,\n",
        "    num_epochs=10,\n",
        "    loss_type='cross_entropy',\n",
        "    weights_path=None,\n",
        "    top_k=3,\n",
        "    output_dir=Path('experiments'),\n",
        "):\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    device = get_device()\n",
        "    base_state_dict = prepare_base_state(weights_path, device) if weights_path else None\n",
        "    results = []\n",
        "    top_models = []\n",
        "    for idx, cfg in enumerate(trials, start=1):\n",
        "        print(f'===== Trial {idx}/{len(trials)} =====')\n",
        "        result = run_single_trial(\n",
        "            cfg,\n",
        "            num_epochs=num_epochs,\n",
        "            loss_type=loss_type,\n",
        "            base_state_dict=base_state_dict,\n",
        "        )\n",
        "        results.append(result)\n",
        "        dice_score = result['best_val_dice']\n",
        "        save_path = output_dir / f\"trial_{idx:03d}_dice_{dice_score:.4f}.pt\"\n",
        "        torch.save(\n",
        "            {\n",
        "                'config': result['config'],\n",
        "                'state_dict': result['best_state_dict'],\n",
        "                'history': result['history'],\n",
        "                'best_val_dice': dice_score,\n",
        "            },\n",
        "            save_path,\n",
        "        )\n",
        "        top_models.append({'score': dice_score, 'path': save_path, 'config': result['config']})\n",
        "        top_models.sort(key=lambda item: item['score'], reverse=True)\n",
        "        while len(top_models) > top_k:\n",
        "            removed = top_models.pop()\n",
        "            if removed['path'].exists():\n",
        "                removed['path'].unlink()\n",
        "        print('Top actual:')\n",
        "        for rank, item in enumerate(top_models, start=1):\n",
        "            print(f\"  {rank}) Dice={item['score']:.4f} -> {item['path'].name}\")\n",
        "    return {'results': results, 'top_models': top_models}\n",
        "\n",
        "# Ejemplo de ejecución:\n",
        "summary = run_trials(random_trials, num_epochs=10, loss_type='cross_entropy', weights_path=WEIGHTS_PATH, top_k=3)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
