{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b78ff95a",
      "metadata": {},
      "source": [
        "# **Proyecto Final Inteligencia Artificial**\n",
        "\n",
        "### Autores: **Angel David Piñeros Sierra**, **Camilo Andrés Roncancio Toca**, **Kelly Johana Solano Calderón**\n",
        "### Presentado a: **Darwin Eduardo Martinez Riaño**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a410ac",
      "metadata": {},
      "source": [
        "## **Modelo de segmentación de imágenes para la localización de lesiones asociadas al cáncer de piel**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb7b245c",
      "metadata": {},
      "source": [
        "### *Glosario*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7598b777",
      "metadata": {},
      "source": [
        "### *(A) Descripción de la problemática*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a0a7714",
      "metadata": {},
      "source": [
        "### *(B) Objetivo*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca1cd0cb",
      "metadata": {},
      "source": [
        "### *(C) Descripción del dataset*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa109c33",
      "metadata": {},
      "source": [
        "El dataset seleccionado para la evaluación del modelo fue el denominado “Skin cancer: HAM10000” de la plataforma de Kaggle, el cual ofrece un conjunto de imágenes especiales para realizar tareas de segmentación y clasificación. Para el propósito de segmentación, el dataset incluye para cada una de las imágenes, el conjunto de máscaras qué determinan la segmentación de las lesiones de cáncer de piel. \n",
        "\n",
        "> El acrónimo HAM10000 significa “Human Against Machine with 10000 training images”. \n",
        "\n",
        "Este dataset es una recopilación de imágenes demoscópicas de diferentes poblaciones. Estas fueron originalmente publicadas inicialmente en el repositorio de Harvard Dataverse,  con el propósito de abordar la dificultad de encontrar un dataset lo suficientemente grande y diverso para realizar diagnósticos automatizados de lesiones cutáneas pigmentadas. \n",
        "\n",
        "El dataset se conforma de dos carpetas: images y masks. Cada una con **10015** imágenes en formato **.JPEG**. Todas las imágenes tienen una dimensión de `600px X 450px`\n",
        "\n",
        "<img src=\"https://res.cloudinary.com/dlsntlruu/image/upload/v1764556079/carpeta_images_pieoyu.png\" width=\"600px\"/>\n",
        "\n",
        "<img src=\"https://res.cloudinary.com/dlsntlruu/image/upload/v1764556079/carpeta_masks_taifwn.png\" width=\"600px\"/>\n",
        "\n",
        "Las imágenes incluyen diagnósticos de:\n",
        "*  Queratosis actínicas\n",
        "*  Carcinoma intraepitelial\n",
        "*  Carcinoma basocelular\n",
        "*  Lesiones de tipo queratosis\n",
        "*  Dermatofibroma\n",
        "*  Melanoma\n",
        "*  Lesiones vasculares\n",
        "\n",
        "Contar con una amplia gama de diagnósticos permite qué la tarea de segmentación semántica pueda realizarse de forma óptima. \n",
        "\n",
        "Para mayor información: \n",
        "\n",
        "*  Skin cancer: HAM10000: https://www.kaggle.com/datasets/surajghuwalewala/ham1000-segmentation-and-classification/\n",
        "*  The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "313944b8",
      "metadata": {},
      "source": [
        "### *(D) Importación y organización de datos*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "558cb2b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (1.8.2)\n",
            "Requirement already satisfied: pandas in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (2.3.3)\n",
            "Requirement already satisfied: torch in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (2.9.1)\n",
            "Requirement already satisfied: PILlow in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (12.0.0)\n",
            "Requirement already satisfied: torchvision in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (0.24.1)\n",
            "Requirement already satisfied: black>=24.10.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (25.11.0)\n",
            "Requirement already satisfied: bleach in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: kagglesdk in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (0.1.13)\n",
            "Requirement already satisfied: mypy>=1.15.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (1.19.0)\n",
            "Requirement already satisfied: protobuf in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (6.33.1)\n",
            "Requirement already satisfied: python-dateutil in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.32.5)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (80.9.0)\n",
            "Requirement already satisfied: six>=1.10 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: types-requests in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.32.4.20250913)\n",
            "Requirement already satisfied: types-tqdm in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (4.67.0.20250809)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from pandas) (2.3.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from torch) (3.5.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (8.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (1.1.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (25.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (4.5.0)\n",
            "Requirement already satisfied: pytokens>=0.3.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (0.3.0)\n",
            "Requirement already satisfied: librt>=0.6.2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from mypy>=1.15.0->kaggle) (0.6.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: webencodings in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from requests->kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from requests->kaggle) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/angel/Bureau/Proyecto IA/.venv/lib/python3.12/site-packages (from requests->kaggle) (2025.11.12)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle pandas torch PILlow torchvision "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "453f4670",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.v2 as v2\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c8077e39",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La carpeta 'ham1000_data' ya existe. No se descargará de nuevo.\n",
            "Contenido de la carpeta: ['GroundTruth.csv', 'images', 'masks']\n",
            "Número de imágenes: 10017\n",
            "Número de máscaras: 10015\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "dataset_url = \"surajghuwalewala/ham1000-segmentation-and-classification\"\n",
        "ruta_descarga = \"ham1000_data\"\n",
        "\n",
        "if os.path.exists(ruta_descarga):\n",
        "    print(f\"La carpeta '{ruta_descarga}' ya existe. No se descargará de nuevo.\")\n",
        "else:\n",
        "    os.makedirs(ruta_descarga, exist_ok=True)\n",
        "    subprocess.run([\n",
        "        \"kaggle\", \"datasets\", \"download\",\n",
        "        \"-d\", dataset_url,\n",
        "        \"-p\", ruta_descarga,\n",
        "        \"--unzip\"\n",
        "    ], check=True)\n",
        "\n",
        "print(\"Contenido de la carpeta:\", os.listdir(ruta_descarga))\n",
        "\n",
        "images_dir = os.path.join(ruta_descarga, \"images\")\n",
        "masks_dir = os.path.join(ruta_descarga, \"masks\")\n",
        "\n",
        "if os.path.isdir(images_dir):\n",
        "    print(\"Número de imágenes:\", len(os.listdir(images_dir)))\n",
        "else:\n",
        "    print(\"No encontré la carpeta 'images'.\")\n",
        "\n",
        "if os.path.isdir(masks_dir):\n",
        "    print(\"Número de máscaras:\", len(os.listdir(masks_dir)))\n",
        "else:\n",
        "    print(\"No encontré la carpeta 'masks'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c09ae392",
      "metadata": {
        "tags": [
          "splits"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 7236 imágenes\n",
            "val: 1276 imágenes\n",
            "test: 1503 imágenes\n",
            "Splits creados en ham1000_data_splits\n"
          ]
        }
      ],
      "source": [
        "DATA_ROOT = Path('ham1000_data')\n",
        "OUTPUT_ROOT = Path('ham1000_data_splits')\n",
        "TRAIN_RATIO = 0.85  # train + val\n",
        "VAL_FROM_TRAIN = 0.15  # porcentaje de train destinado a validación\n",
        "SEED = 42\n",
        "MASK_SUFFIX = '_segmentation.png'\n",
        "\n",
        "image_dir = DATA_ROOT / 'images'\n",
        "mask_dir = DATA_ROOT / 'masks'\n",
        "if not image_dir.exists() or not mask_dir.exists():\n",
        "    raise RuntimeError(f\"No se hallaron carpetas esperadas en {DATA_ROOT}.\")\n",
        "\n",
        "random.seed(SEED)\n",
        "if OUTPUT_ROOT.exists():\n",
        "    shutil.rmtree(OUTPUT_ROOT)\n",
        "for split in ('train', 'val', 'test'):\n",
        "    (OUTPUT_ROOT / split / 'images').mkdir(parents=True, exist_ok=True)\n",
        "    (OUTPUT_ROOT / split / 'masks').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "allowed_exts = {'.png', '.jpg', '.jpeg'}\n",
        "image_paths = [p for p in image_dir.iterdir() if p.suffix.lower() in allowed_exts]\n",
        "if not image_paths:\n",
        "    raise RuntimeError(f'No se encontraron imágenes en {image_dir}.')\n",
        "image_paths.sort()\n",
        "random.shuffle(image_paths)\n",
        "\n",
        "total = len(image_paths)\n",
        "train_val_count = int(total * TRAIN_RATIO)\n",
        "val_count = int(train_val_count * VAL_FROM_TRAIN)\n",
        "train_count = train_val_count - val_count\n",
        "\n",
        "splits = {\n",
        "    'train': image_paths[:train_count],\n",
        "    'val': image_paths[train_count: train_count + val_count],\n",
        "    'test': image_paths[train_count + val_count:]\n",
        "}\n",
        "\n",
        "for split_name, files in splits.items():\n",
        "    dst_img = OUTPUT_ROOT / split_name / 'images'\n",
        "    dst_mask = OUTPUT_ROOT / split_name / 'masks'\n",
        "    for img_path in files:\n",
        "        mask_path = mask_dir / f\"{img_path.stem}{MASK_SUFFIX}\"\n",
        "        if not mask_path.exists():\n",
        "            raise FileNotFoundError(f'Falta máscara: {mask_path}')\n",
        "        shutil.copy2(img_path, dst_img / img_path.name)\n",
        "        shutil.copy2(mask_path, dst_mask / mask_path.name)\n",
        "    print(f\"{split_name}: {len(files)} imágenes\")\n",
        "print('Splits creados en', OUTPUT_ROOT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e1e979a2",
      "metadata": {
        "tags": [
          "train_stats"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Media por canal (reescala 256x256): [0.7633206844329834, 0.5454925894737244, 0.5698098540306091]\n",
            "Desviación estándar por canal: [0.08873128145933151, 0.11731848120689392, 0.13180507719516754]\n"
          ]
        }
      ],
      "source": [
        "TRAIN_IMG_DIR = Path('ham1000_data_splits/train/images')\n",
        "if not TRAIN_IMG_DIR.exists():\n",
        "    raise RuntimeError(f'No existe el directorio: {TRAIN_IMG_DIR}')\n",
        "\n",
        "TARGET_SIZE = (256, 256)\n",
        "resize_transform = transforms.Resize(TARGET_SIZE, interpolation=InterpolationMode.BILINEAR)\n",
        "\n",
        "img_files = sorted(\n",
        "    [p for p in TRAIN_IMG_DIR.iterdir() if p.suffix.lower() in {'.png', '.jpg', '.jpeg'}]\n",
        ")\n",
        "if not img_files:\n",
        "    raise RuntimeError(f'No se encontraron imágenes en {TRAIN_IMG_DIR}')\n",
        "\n",
        "means, stds = [], []\n",
        "for img_path in img_files:\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_tensor = transforms.ToTensor()(resize_transform(img))\n",
        "    means.append(img_tensor.mean(dim=(1, 2)))\n",
        "    stds.append(img_tensor.std(dim=(1, 2)))\n",
        "\n",
        "IMG_MEAN = torch.stack(means).mean(dim=0)\n",
        "IMG_STD = torch.stack(stds).mean(dim=0)\n",
        "print(f'Media por canal (reescala {TARGET_SIZE[0]}x{TARGET_SIZE[1]}): {IMG_MEAN.tolist()}')\n",
        "print(f'Desviación estándar por canal: {IMG_STD.tolist()}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0461c7f5",
      "metadata": {
        "tags": [
          "transforms"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformaciones definidas:\n",
            "train_joint_transform: Compose(\n",
            "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
            "      RandomHorizontalFlip(p=0.5)\n",
            "      RandomVerticalFlip(p=0.2)\n",
            "      RandomRotation(degrees=[-180.0, 180.0], interpolation=InterpolationMode.NEAREST, expand=False, fill=0)\n",
            "      ElasticTransform(alpha=[40.0, 40.0], sigma=[5.0, 5.0], interpolation=InterpolationMode.BILINEAR, fill=0)\n",
            "      ColorJitter(brightness=(0.85, 1.15), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.02, 0.02))\n",
            "      GaussianBlur(kernel_size=(3, 3), sigma=[0.1, 1.0])\n",
            "      ToDtype(scale=True)\n",
            "      Normalize(mean=[tensor(0.7633), tensor(0.5455), tensor(0.5698)], std=[tensor(0.0887), tensor(0.1173), tensor(0.1318)], inplace=False)\n",
            ")\n",
            "val_test_joint_transform: Compose(\n",
            "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
            "      ToDtype(scale=True)\n",
            "      Normalize(mean=[tensor(0.7633), tensor(0.5455), tensor(0.5698)], std=[tensor(0.0887), tensor(0.1173), tensor(0.1318)], inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "normalize_transform = v2.Normalize(mean=IMG_MEAN, std=IMG_STD)\n",
        "\n",
        "train_joint_transform = v2.Compose([\n",
        "    v2.Resize(TARGET_SIZE, interpolation=InterpolationMode.BILINEAR, antialias=True),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.RandomVerticalFlip(p=0.2),\n",
        "    v2.RandomRotation(degrees=(-180, 180)),\n",
        "    v2.ElasticTransform(alpha=40.0, sigma=5.0, interpolation=InterpolationMode.BILINEAR),\n",
        "    v2.ColorJitter(brightness=0.15, contrast=0.2, saturation=0.2, hue=0.02),\n",
        "    v2.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    normalize_transform,\n",
        "])\n",
        "\n",
        "val_test_joint_transform = v2.Compose([\n",
        "    v2.Resize(TARGET_SIZE, interpolation=InterpolationMode.BILINEAR, antialias=True),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    normalize_transform,\n",
        "])\n",
        "\n",
        "def apply_train_transforms(image, mask):\n",
        "    image_tv = tv_tensors.Image(image)\n",
        "    mask_tv = tv_tensors.Mask(mask)\n",
        "    image_aug, mask_aug = train_joint_transform(image_tv, mask_tv)\n",
        "    return image_aug, mask_aug\n",
        "\n",
        "def apply_val_transforms(image, mask):\n",
        "    image_tv = tv_tensors.Image(image)\n",
        "    mask_tv = tv_tensors.Mask(mask)\n",
        "    image_val, mask_val = val_test_joint_transform(image_tv, mask_tv)\n",
        "    return image_val, mask_val\n",
        "\n",
        "def apply_test_transforms(image, mask):\n",
        "    image_tv = tv_tensors.Image(image)\n",
        "    mask_tv = tv_tensors.Mask(mask)\n",
        "    image_test, mask_test = val_test_joint_transform(image_tv, mask_tv)\n",
        "    return image_test, mask_test\n",
        "\n",
        "print(\"Transformaciones definidas:\")\n",
        "print(\"train_joint_transform:\", train_joint_transform)\n",
        "print(\"val_test_joint_transform:\", val_test_joint_transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c73fa70f",
      "metadata": {
        "tags": [
          "dataset"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 7236 muestras\n",
            "val: 1276 muestras\n",
            "test: 1503 muestras\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class HAM1000SegmentationDataset(Dataset):\n",
        "    def __init__(self, root_dir: Path, split: str, transform_fn=None, mask_suffix: str = '_segmentation.png'):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        if split not in {'train', 'val', 'test'}:\n",
        "            raise ValueError(\"split debe ser 'train', 'val' o 'test'\")\n",
        "        self.split = split\n",
        "        self.images_dir = self.root_dir / split / 'images'\n",
        "        self.masks_dir = self.root_dir / split / 'masks'\n",
        "        if not self.images_dir.exists() or not self.masks_dir.exists():\n",
        "            raise RuntimeError(f'No se hallan carpetas para el split {split} en {self.root_dir}')\n",
        "        self.mask_suffix = mask_suffix\n",
        "        self.transform_fn = transform_fn\n",
        "\n",
        "        allowed_exts = {'.png', '.jpg', '.jpeg'}\n",
        "        self.samples = []\n",
        "        for img_path in sorted(self.images_dir.iterdir()):\n",
        "            if img_path.suffix.lower() not in allowed_exts:\n",
        "                continue\n",
        "            mask_path = self.masks_dir / f\"{img_path.stem}{self.mask_suffix}\"\n",
        "            if not mask_path.exists():\n",
        "                continue\n",
        "            self.samples.append((img_path, mask_path))\n",
        "        if not self.samples:\n",
        "            raise RuntimeError(f'No se encontraron pares imagen-máscara en {self.images_dir}')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        if self.transform_fn is not None:\n",
        "            image, mask = self.transform_fn(image, mask)\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "def get_transform_fn(split: str):\n",
        "    if split == 'train':\n",
        "        return apply_train_transforms\n",
        "    if split == 'val':\n",
        "        return apply_val_transforms\n",
        "    if split == 'test':\n",
        "        return apply_test_transforms\n",
        "    raise ValueError('split desconocido')\n",
        "\n",
        "\n",
        "def create_datasets(root_dir: Path = Path('ham1000_data_splits')):\n",
        "    datasets = {}\n",
        "    for split in ('train', 'val', 'test'):\n",
        "        datasets[split] = HAM1000SegmentationDataset(\n",
        "            root_dir=root_dir,\n",
        "            split=split,\n",
        "            transform_fn=get_transform_fn(split),\n",
        "        )\n",
        "        print(f\"{split}: {len(datasets[split])} muestras\")\n",
        "    return datasets\n",
        "\n",
        "ham_datasets = create_datasets()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "add8745d",
      "metadata": {
        "tags": [
          "dataloader"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaders listos:\n",
            "train: 905 batches\n",
            "val: 160 batches\n",
            "test: 188 batches\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 4\n",
        "PIN_MEMORY = True\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ham_datasets['train'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    ham_datasets['val'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    ham_datasets['test'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        ")\n",
        "\n",
        "print('Loaders listos:')\n",
        "print(f\"train: {len(train_loader)} batches\")\n",
        "print(f\"val: {len(val_loader)} batches\")\n",
        "print(f\"test: {len(test_loader)} batches\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
